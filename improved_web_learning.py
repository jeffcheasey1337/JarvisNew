# -*- coding: utf-8 -*-
"""
üåê IMPROVED AUTONOMOUS WEB LEARNING SYSTEM v2.0
–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –æ–±—Ö–æ–¥–æ–º –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫

–ò—Å—Ç–æ—á–Ω–∏–∫–∏:
‚úÖ Wikipedia API (—Ä—É—Å—Å–∫–∞—è + –∞–Ω–≥–ª–∏–π—Å–∫–∞—è)
‚úÖ Specialized search libraries
‚úÖ Multiple fallback options
‚úÖ Anti-blocking measures
"""

import logging
import requests
from bs4 import BeautifulSoup
import time
import random
from urllib.parse import quote_plus
import re
from pathlib import Path
import json

logger = logging.getLogger(__name__)


class ImprovedWebCollector:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–±–æ—Ä—â–∏–∫ –∑–Ω–∞–Ω–∏–π"""
    
    def __init__(self):
        self.session = requests.Session()
        
        # –†–æ—Ç–∞—Ü–∏—è User-Agents
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        ]
        
        self.stats = {
            'topics_processed': 0,
            'sources_collected': 0,
            'chars_collected': 0,
            'errors': 0,
        }
    
    def _get_random_ua(self):
        """–°–ª—É—á–∞–π–Ω—ã–π User-Agent"""
        return random.choice(self.user_agents)
    
    def search_wikipedia(self, query, lang='ru'):
        """
        –ü–æ–∏—Å–∫ –≤ Wikipedia —á–µ—Ä–µ–∑ API
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            lang: –Ø–∑—ã–∫ ('ru' –∏–ª–∏ 'en')
            
        Returns:
            Dict —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        """
        try:
            # Wikipedia API
            api_url = f"https://{lang}.wikipedia.org/w/api.php"
            
            # 1. –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π
            search_params = {
                'action': 'opensearch',
                'search': query,
                'limit': 3,
                'format': 'json'
            }
            
            response = self.session.get(api_url, params=search_params, timeout=10)
            response.raise_for_status()
            
            search_results = response.json()
            
            if len(search_results) < 2 or not search_results[1]:
                return None
            
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            title = search_results[1][0]
            
            # 2. –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏
            content_params = {
                'action': 'query',
                'prop': 'extracts',
                'exintro': True,  # –¢–æ–ª—å–∫–æ –≤–≤–µ–¥–µ–Ω–∏–µ
                'explaintext': True,  # –¢–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç
                'titles': title,
                'format': 'json'
            }
            
            response = self.session.get(api_url, params=content_params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            pages = data.get('query', {}).get('pages', {})
            
            for page_id, page_data in pages.items():
                extract = page_data.get('extract', '')
                
                if extract and len(extract) > 100:
                    return {
                        'source': f'Wikipedia ({lang})',
                        'title': title,
                        'url': f"https://{lang}.wikipedia.org/wiki/{title.replace(' ', '_')}",
                        'content': extract,
                        'lang': lang
                    }
            
            return None
        
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ Wikipedia {lang}: {e}")
            return None
    
    def search_simple_wikipedia(self, query):
        """
        Simple Wikipedia (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        –û—Ç–ª–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        """
        try:
            api_url = "https://simple.wikipedia.org/w/api.php"
            
            # –ü–æ–∏—Å–∫
            search_params = {
                'action': 'opensearch',
                'search': query,
                'limit': 1,
                'format': 'json'
            }
            
            response = self.session.get(api_url, params=search_params, timeout=10)
            response.raise_for_status()
            
            results = response.json()
            
            if len(results) < 2 or not results[1]:
                return None
            
            title = results[1][0]
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            content_params = {
                'action': 'query',
                'prop': 'extracts',
                'exintro': True,
                'explaintext': True,
                'titles': title,
                'format': 'json'
            }
            
            response = self.session.get(api_url, params=content_params, timeout=10)
            data = response.json()
            
            pages = data.get('query', {}).get('pages', {})
            
            for page_data in pages.values():
                extract = page_data.get('extract', '')
                
                if extract and len(extract) > 50:
                    return {
                        'source': 'Simple Wikipedia',
                        'title': title,
                        'url': f"https://simple.wikipedia.org/wiki/{title.replace(' ', '_')}",
                        'content': extract,
                        'lang': 'simple'
                    }
        
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ Simple Wikipedia: {e}")
        
        return None
    
    def search_wikidata(self, query):
        """
        Wikidata - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        """
        try:
            api_url = "https://www.wikidata.org/w/api.php"
            
            # –ü–æ–∏—Å–∫ —Å—É—â–Ω–æ—Å—Ç–∏
            params = {
                'action': 'wbsearchentities',
                'search': query,
                'language': 'ru',
                'limit': 1,
                'format': 'json'
            }
            
            response = self.session.get(api_url, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            if 'search' in data and data['search']:
                entity = data['search'][0]
                
                description = entity.get('description', '')
                label = entity.get('label', '')
                
                if description:
                    return {
                        'source': 'Wikidata',
                        'title': label,
                        'content': f"{label}: {description}",
                        'url': f"https://www.wikidata.org/wiki/{entity['id']}"
                    }
        
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ Wikidata: {e}")
        
        return None
    
    def collect_knowledge(self, topic):
        """
        –°–±–æ—Ä –∑–Ω–∞–Ω–∏–π –ø–æ —Ç–µ–º–µ –∏–∑ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        
        Args:
            topic: –¢–µ–º–∞ –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è
            
        Returns:
            Dict —Å —Å–æ–±—Ä–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏
        """
        logger.info(f"üìö –°–±–æ—Ä –∑–Ω–∞–Ω–∏–π: {topic}")
        
        knowledge = {
            'topic': topic,
            'sources': [],
            'content': [],
            'total_chars': 0,
        }
        
        try:
            # 1. –†—É—Å—Å–∫–∞—è Wikipedia
            logger.debug("–ü–æ–∏—Å–∫ –≤ –†—É—Å—Å–∫–æ–π Wikipedia...")
            wiki_ru = self.search_wikipedia(topic, lang='ru')
            
            if wiki_ru:
                knowledge['sources'].append(wiki_ru)
                knowledge['content'].append(wiki_ru['content'])
                logger.info(f"‚úì Wikipedia RU: {len(wiki_ru['content'])} —Å–∏–º–≤–æ–ª–æ–≤")
                time.sleep(0.5)
            
            # 2. –ê–Ω–≥–ª–∏–π—Å–∫–∞—è Wikipedia
            logger.debug("–ü–æ–∏—Å–∫ –≤ –ê–Ω–≥–ª–∏–π—Å–∫–æ–π Wikipedia...")
            wiki_en = self.search_wikipedia(topic, lang='en')
            
            if wiki_en:
                knowledge['sources'].append(wiki_en)
                knowledge['content'].append(wiki_en['content'])
                logger.info(f"‚úì Wikipedia EN: {len(wiki_en['content'])} —Å–∏–º–≤–æ–ª–æ–≤")
                time.sleep(0.5)
            
            # 3. Simple Wikipedia
            logger.debug("–ü–æ–∏—Å–∫ –≤ Simple Wikipedia...")
            wiki_simple = self.search_simple_wikipedia(topic)
            
            if wiki_simple:
                knowledge['sources'].append(wiki_simple)
                knowledge['content'].append(wiki_simple['content'])
                logger.info(f"‚úì Simple Wikipedia: {len(wiki_simple['content'])} —Å–∏–º–≤–æ–ª–æ–≤")
                time.sleep(0.5)
            
            # 4. Wikidata
            logger.debug("–ü–æ–∏—Å–∫ –≤ Wikidata...")
            wikidata = self.search_wikidata(topic)
            
            if wikidata:
                knowledge['sources'].append(wikidata)
                knowledge['content'].append(wikidata['content'])
                logger.info(f"‚úì Wikidata: {len(wikidata['content'])} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ü–æ–¥—Å—á–µ—Ç
            knowledge['total_chars'] = sum(len(c) for c in knowledge['content'])
            
            self.stats['topics_processed'] += 1
            self.stats['sources_collected'] += len(knowledge['sources'])
            self.stats['chars_collected'] += knowledge['total_chars']
            
            logger.info(f"üìä –ò—Ç–æ–≥–æ: {len(knowledge['sources'])} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, {knowledge['total_chars']} —Å–∏–º–≤–æ–ª–æ–≤")
            
            return knowledge
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –∑–Ω–∞–Ω–∏–π '{topic}': {e}")
            self.stats['errors'] += 1
            return knowledge


class ImprovedAutonomousLearning:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∞–≤—Ç–æ–Ω–æ–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, memory_system=None, turbo_system=None):
        self.memory_system = memory_system
        self.turbo_system = turbo_system
        
        # –ö–æ–ª–ª–µ–∫—Ç–æ—Ä
        self.collector = ImprovedWebCollector()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'topics_learned': 0,
            'total_content': 0,
            'embeddings_created': 0,
        }
        
        # –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        self.data_dir = Path('data/knowledge')
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –≥–æ—Ç–æ–≤–∞")
    
    def learn_topic(self, topic):
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–µ–º–µ
        
        Args:
            topic: –¢–µ–º–∞ –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è
            
        Returns:
            Success status
        """
        try:
            logger.info(f"üéì –û–±—É—á–µ–Ω–∏–µ: {topic}")
            
            # 1. –°–æ–±–∏—Ä–∞–µ–º –∑–Ω–∞–Ω–∏—è
            knowledge = self.collector.collect_knowledge(topic)
            
            if not knowledge['content']:
                logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è '{topic}'")
                return False
            
            # 2. –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            full_content = "\n\n".join(knowledge['content'])
            
            logger.info(f"üìù –°–æ–±—Ä–∞–Ω–æ {len(knowledge['content'])} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
            
            # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º
            self._save_knowledge(topic, knowledge)
            
            # 4. –°–æ–∑–¥–∞–µ–º embeddings
            if self.turbo_system:
                try:
                    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
                    chunks = self._split_content(full_content, max_size=2000)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–µ–º—ã
                    chunks_with_context = [f"{topic}: {chunk}" for chunk in chunks]
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ GPU
                    result = self.turbo_system.learn_batch(
                        chunks_with_context,
                        category="web_learning"
                    )
                    
                    self.stats['embeddings_created'] += len(chunks)
                    
                    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} embeddings")
                
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è embeddings: {e}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            self.stats['topics_learned'] += 1
            self.stats['total_content'] += knowledge['total_chars']
            
            logger.info(f"‚úÖ –¢–µ–º–∞ '{topic}' –∏–∑—É—á–µ–Ω–∞!")
            
            return True
        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è '{topic}': {e}")
            return False
    
    def _split_content(self, content, max_size=2000):
        """–†–∞–∑–±–∏–≤–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        chunks = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º
        paragraphs = content.split('\n\n')
        
        current_chunk = ""
        
        for para in paragraphs:
            if len(current_chunk) + len(para) < max_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para + "\n\n"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _save_knowledge(self, topic, knowledge):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –≤ —Ñ–∞–π–ª"""
        try:
            filename = self._sanitize_filename(topic) + '.json'
            filepath = self.data_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(knowledge, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filepath}")
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
    
    def _sanitize_filename(self, text):
        """–û—á–∏—Å—Ç–∫–∞ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
        text = re.sub(r'[<>:"/\\|?*]', '_', text)
        return text[:100]
    
    def batch_learn(self, topics_list, delay=2):
        """
        –ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        
        Args:
            topics_list: –°–ø–∏—Å–æ–∫ —Ç–µ–º
            delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏ (—Å–µ–∫)
        """
        logger.info(f"üìö –ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: {len(topics_list)} —Ç–µ–º")
        
        for i, topic in enumerate(topics_list):
            logger.info(f"[{i+1}/{len(topics_list)}] {topic}")
            
            success = self.learn_topic(topic)
            
            if not success:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å—Å—è: {topic}")
            
            # –ü–∞—É–∑–∞
            if i < len(topics_list) - 1:
                time.sleep(delay)
        
        logger.info("="*80)
        logger.info("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        logger.info("="*80)
        logger.info(f"–¢–µ–º –∏–∑—É—á–µ–Ω–æ: {self.stats['topics_learned']}/{len(topics_list)}")
        logger.info(f"–ö–æ–Ω—Ç–µ–Ω—Ç–∞ —Å–æ–±—Ä–∞–Ω–æ: {self.stats['total_content']} —Å–∏–º–≤–æ–ª–æ–≤")
        logger.info(f"Embeddings —Å–æ–∑–¥–∞–Ω–æ: {self.stats['embeddings_created']}")
        logger.info("="*80)
    
    def get_stats(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        return {
            **self.stats,
            'collector': self.collector.stats
        }


# –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫
if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    print("="*80)
    print("üåê IMPROVED WEB LEARNING SYSTEM v2.0 - TEST")
    print("="*80)
    print()
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏—Å—Ç–µ–º—É
    system = ImprovedAutonomousLearning()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–º—ã
    test_topics = [
        "Python",
        "–ö–≤–µ–Ω—Ç–∏–Ω –¢–∞—Ä–∞–Ω—Ç–∏–Ω–æ",
        "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
        "Sex Pistols",
    ]
    
    print(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ {len(test_topics)} —Ç–µ–º–∞—Ö")
    print()
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º
    system.batch_learn(test_topics, delay=1)
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = system.get_stats()
    
    print()
    print("–§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"  –£—Å–ø–µ—à–Ω–æ –∏–∑—É—á–µ–Ω–æ: {stats['topics_learned']}/{len(test_topics)}")
    print(f"  –ö–æ–Ω—Ç–µ–Ω—Ç: {stats['total_content']} —Å–∏–º–≤–æ–ª–æ–≤ ({stats['total_content']/1024:.1f} KB)")
    print(f"  Embeddings: {stats['embeddings_created']}")
    print(f"  –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {stats['collector']['sources_collected']}")
    print()
