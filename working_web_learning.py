# -*- coding: utf-8 -*-
"""
‚úÖ WORKING WEB LEARNING v3.0
–° –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º User-Agent –¥–ª—è Wikipedia
"""

import logging
import requests
import time
import re
from pathlib import Path
import json

logger = logging.getLogger(__name__)


class WorkingWikipediaCollector:
    """–°–±–æ—Ä—â–∏–∫ –∏–∑ Wikipedia —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º User-Agent"""
    
    def __init__(self):
        self.session = requests.Session()
        
        # –ü–†–ê–í–ò–õ–¨–ù–´–ô User-Agent –¥–ª—è Wikipedia
        self.session.headers.update({
            'User-Agent': 'JARVIS-Learning/1.0 (Educational Project; Python/3.11) requests/2.31.0'
        })
        
        self.stats = {
            'topics_processed': 0,
            'sources_collected': 0,
            'chars_collected': 0,
        }
    
    def search_wikipedia(self, query, lang='ru'):
        """–ü–æ–∏—Å–∫ –≤ Wikipedia"""
        try:
            api_url = f"https://{lang}.wikipedia.org/w/api.php"
            
            # –ü–æ–∏—Å–∫
            search_params = {
                'action': 'opensearch',
                'search': query,
                'limit': 3,
                'format': 'json'
            }
            
            response = self.session.get(api_url, params=search_params, timeout=15)
            response.raise_for_status()
            
            results = response.json()
            
            if len(results) < 2 or not results[1]:
                return None
            
            title = results[1][0]
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_params = {
                'action': 'query',
                'prop': 'extracts',
                'exintro': True,
                'explaintext': True,
                'titles': title,
                'format': 'json'
            }
            
            response = self.session.get(api_url, params=content_params, timeout=15)
            response.raise_for_status()
            
            data = response.json()
            pages = data.get('query', {}).get('pages', {})
            
            for page_data in pages.values():
                extract = page_data.get('extract', '')
                
                if extract and len(extract) > 100:
                    return {
                        'source': f'Wikipedia ({lang})',
                        'title': title,
                        'url': f"https://{lang}.wikipedia.org/wiki/{title.replace(' ', '_')}",
                        'content': extract,
                        'lang': lang
                    }
            
            return None
        
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ Wikipedia {lang}: {e}")
            return None
    
    def collect_knowledge(self, topic):
        """–°–±–æ—Ä –∑–Ω–∞–Ω–∏–π –ø–æ —Ç–µ–º–µ"""
        logger.info(f"üìö –°–±–æ—Ä: {topic}")
        
        knowledge = {
            'topic': topic,
            'sources': [],
            'content': [],
            'total_chars': 0,
        }
        
        # –†—É—Å—Å–∫–∞—è Wikipedia
        wiki_ru = self.search_wikipedia(topic, lang='ru')
        if wiki_ru:
            knowledge['sources'].append(wiki_ru)
            knowledge['content'].append(wiki_ru['content'])
            logger.info(f"‚úì Wikipedia RU: {len(wiki_ru['content'])} —Å–∏–º–≤–æ–ª–æ–≤")
            time.sleep(1)
        
        # –ê–Ω–≥–ª–∏–π—Å–∫–∞—è Wikipedia
        wiki_en = self.search_wikipedia(topic, lang='en')
        if wiki_en:
            knowledge['sources'].append(wiki_en)
            knowledge['content'].append(wiki_en['content'])
            logger.info(f"‚úì Wikipedia EN: {len(wiki_en['content'])} —Å–∏–º–≤–æ–ª–æ–≤")
            time.sleep(1)
        
        knowledge['total_chars'] = sum(len(c) for c in knowledge['content'])
        
        self.stats['topics_processed'] += 1
        self.stats['sources_collected'] += len(knowledge['sources'])
        self.stats['chars_collected'] += knowledge['total_chars']
        
        if knowledge['sources']:
            logger.info(f"üìä –ò—Ç–æ–≥–æ: {len(knowledge['sources'])} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, {knowledge['total_chars']} —Å–∏–º–≤–æ–ª–æ–≤")
        
        return knowledge


class WorkingLearningSystem:
    """–†–∞–±–æ—á–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, turbo_system=None):
        self.turbo_system = turbo_system
        self.collector = WorkingWikipediaCollector()
        
        self.stats = {
            'topics_learned': 0,
            'total_content': 0,
            'embeddings_created': 0,
        }
        
        self.data_dir = Path('data/knowledge')
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞")
    
    def learn_topic(self, topic):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–µ–º–µ"""
        try:
            logger.info(f"üéì –û–±—É—á–µ–Ω–∏–µ: {topic}")
            
            knowledge = self.collector.collect_knowledge(topic)
            
            if not knowledge['content']:
                logger.warning(f"‚ö† –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è '{topic}'")
                return False
            
            full_content = "\n\n".join(knowledge['content'])
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            self._save(topic, knowledge)
            
            # Embeddings
            if self.turbo_system:
                try:
                    chunks = self._split(full_content)
                    chunks_ctx = [f"{topic}: {c}" for c in chunks]
                    
                    self.turbo_system.learn_batch(chunks_ctx, category="web")
                    
                    self.stats['embeddings_created'] += len(chunks)
                    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} embeddings")
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ embeddings: {e}")
            
            self.stats['topics_learned'] += 1
            self.stats['total_content'] += knowledge['total_chars']
            
            logger.info(f"‚úÖ '{topic}' –∏–∑—É—á–µ–Ω–∞!")
            return True
        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return False
    
    def _split(self, content, max_size=2000):
        """–†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        chunks = []
        paragraphs = content.split('\n\n')
        current = ""
        
        for para in paragraphs:
            if len(current) + len(para) < max_size:
                current += para + "\n\n"
            else:
                if current:
                    chunks.append(current.strip())
                current = para + "\n\n"
        
        if current:
            chunks.append(current.strip())
        
        return chunks
    
    def _save(self, topic, knowledge):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ"""
        try:
            filename = re.sub(r'[<>:"/\\|?*]', '_', topic)[:100] + '.json'
            filepath = self.data_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(knowledge, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
    
    def batch_learn(self, topics, delay=2):
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
        logger.info(f"üìö –û–±—É—á–µ–Ω–∏–µ: {len(topics)} —Ç–µ–º")
        
        for i, topic in enumerate(topics):
            logger.info(f"[{i+1}/{len(topics)}] {topic}")
            self.learn_topic(topic)
            
            if i < len(topics) - 1:
                time.sleep(delay)
        
        logger.info("="*80)
        logger.info(f"–ò–∑—É—á–µ–Ω–æ: {self.stats['topics_learned']}/{len(topics)}")
        logger.info(f"–ö–æ–Ω—Ç–µ–Ω—Ç: {self.stats['total_content']} —Å–∏–º–≤–æ–ª–æ–≤")
        logger.info(f"Embeddings: {self.stats['embeddings_created']}")
        logger.info("="*80)
    
    def get_stats(self):
        return {**self.stats, 'collector': self.collector.stats}


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')
    
    print("="*80)
    print("‚úÖ WORKING WEB LEARNING v3.0 - TEST")
    print("="*80)
    print()
    
    system = WorkingLearningSystem()
    
    test_topics = ["Python", "–ö–≤–µ–Ω—Ç–∏–Ω –¢–∞—Ä–∞–Ω—Ç–∏–Ω–æ"]
    
    system.batch_learn(test_topics, delay=2)
    
    stats = system.get_stats()
    print()
    print(f"–£—Å–ø–µ—à–Ω–æ: {stats['topics_learned']}/{len(test_topics)}")
    print(f"–ö–æ–Ω—Ç–µ–Ω—Ç: {stats['total_content']/1024:.1f} KB")
    print(f"–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {stats['collector']['sources_collected']}")
    print()
