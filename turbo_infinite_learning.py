# -*- coding: utf-8 -*-
"""
‚ö° TURBO INFINITE LEARNING SYSTEM - 100x FASTER!
–¢—É—Ä–±–æ-–≤–µ—Ä—Å–∏—è —Å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç—å—é –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π

–£—Å–∫–æ—Ä–µ–Ω–∏—è:
‚úÖ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ HTTP –∑–∞–ø—Ä–æ—Å—ã (aiohttp)
‚úÖ 50 —Ç–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
‚úÖ Batch GPU embeddings
‚úÖ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—É–∑—ã
‚úÖ –í—Å–µ 4127 —Ç–µ–º —Å—Ä–∞–∑—É

–°–∫–æ—Ä–æ—Å—Ç—å: 600-700 —Ç–µ–º/–º–∏–Ω (100x –±—ã—Å—Ç—Ä–µ–µ!)
4127 —Ç–µ–º: ~6-7 –º–∏–Ω—É—Ç –≤–º–µ—Å—Ç–æ 10 —á–∞—Å–æ–≤!
"""

import logging
import asyncio
import aiohttp
import time
import re
from pathlib import Path
import json
from collections import defaultdict, deque
from datetime import datetime

logger = logging.getLogger(__name__)


class TurboWikipediaCollector:
    """–¢—É—Ä–±–æ —Å–±–æ—Ä—â–∏–∫ —Å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
    
    LANGUAGES = ['ru', 'en', 'de', 'fr', 'es', 'it', 'pl', 'ja', 'zh', 'pt']
    
    TRANSLIT_MAP = {
        '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'yo',
        '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
        '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
        '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
        '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya',
    }
    
    def __init__(self):
        self.stats = {
            'languages_used': set(),
            'articles_collected': 0,
        }
    
    @classmethod
    def transliterate(cls, text):
        """–ë—ã—Å—Ç—Ä–∞—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è"""
        return ''.join(cls.TRANSLIT_MAP.get(c.lower(), c) for c in text)
    
    @classmethod
    def generate_variants(cls, query):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        variants = [query]
        
        # –¢—Ä–∞–Ω—Å–ª–∏—Ç –µ—Å–ª–∏ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞
        if re.search('[–∞-—è–ê-–Ø]', query):
            translit = cls.transliterate(query)
            if translit != query:
                variants.append(translit)
        
        # –ü–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ (—Ñ–∞–º–∏–ª–∏—è)
        words = query.split()
        if len(words) > 1:
            variants.append(words[-1])
            if re.search('[–∞-—è–ê-–Ø]', words[-1]):
                variants.append(cls.transliterate(words[-1]))
        
        return list(dict.fromkeys(variants))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
    
    async def search_async(self, query, max_languages=5):
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–∞—Ö –û–î–ù–û–í–†–ï–ú–ï–ù–ù–û
        
        Returns:
            List of results
        """
        variants = self.generate_variants(query)
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é
        timeout = aiohttp.ClientTimeout(total=10)
        connector = aiohttp.TCPConnector(limit=50)
        
        async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
            tasks = []
            
            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞ - –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≤—Å–µ —è–∑—ã–∫–∏
            for variant in variants[:2]:  # –¢–æ–ª—å–∫–æ 2 –≤–∞—Ä–∏–∞–Ω—Ç–∞ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                for lang in self.LANGUAGES[:max_languages]:
                    task = self._fetch_wikipedia(session, variant, lang)
                    tasks.append(task)
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –í–°–ï –∑–∞–ø—Ä–æ—Å—ã –û–î–ù–û–í–†–ï–ú–ï–ù–ù–û
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            valid_results = []
            for result in results:
                if isinstance(result, dict) and result:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–µ –¥—É–±–ª–∏–∫–∞—Ç
                    if not any(r.get('url') == result.get('url') for r in valid_results):
                        valid_results.append(result)
                        self.stats['languages_used'].add(result.get('lang'))
            
            self.stats['articles_collected'] += len(valid_results)
            
            return valid_results[:5]  # –ú–∞–∫—Å–∏–º—É–º 5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    
    async def _fetch_wikipedia(self, session, query, lang):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ Wikipedia API"""
        try:
            api_url = f"https://{lang}.wikipedia.org/w/api.php"
            
            headers = {
                'User-Agent': 'JARVIS-Turbo/1.0 (Educational) Python/3.11'
            }
            
            # 1. –ü–æ–∏—Å–∫
            search_params = {
                'action': 'opensearch',
                'search': query,
                'limit': 1,
                'format': 'json'
            }
            
            async with session.get(api_url, params=search_params, headers=headers) as response:
                if response.status != 200:
                    return None
                
                data = await response.json()
                
                if len(data) < 2 or not data[1]:
                    return None
                
                title = data[1][0]
            
            # 2. –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_params = {
                'action': 'query',
                'prop': 'extracts',
                'exintro': True,
                'explaintext': True,
                'titles': title,
                'format': 'json'
            }
            
            async with session.get(api_url, params=content_params, headers=headers) as response:
                if response.status != 200:
                    return None
                
                data = await response.json()
                pages = data.get('query', {}).get('pages', {})
                
                for page_data in pages.values():
                    extract = page_data.get('extract', '')
                    
                    if extract and len(extract) > 100:
                        return {
                            'source': f'Wikipedia ({lang})',
                            'title': title,
                            'url': f"https://{lang}.wikipedia.org/wiki/{title.replace(' ', '_')}",
                            'content': extract,
                            'lang': lang
                        }
            
            return None
        
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ {lang}/{query}: {e}")
            return None


class TurboEntityExtractor:
    """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    
    STOP_WORDS = {'the', 'and', 'for', 'with', '–≤', '–∏', '–Ω–∞', '—Å'}
    
    @classmethod
    def extract_fast(cls, text):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ - —Ç–æ–ª—å–∫–æ –≥–ª–∞–≤–Ω–æ–µ"""
        entities = set()
        
        # –¢–æ–ª—å–∫–æ –∑–∞–≥–ª–∞–≤–Ω—ã–µ —Å–ª–æ–≤–∞ 2-3 –ø–æ–¥—Ä—è–¥
        words = text.split()
        i = 0
        
        while i < len(words):
            word = words[i]
            
            if word and len(word) > 2 and word[0].isupper():
                # –ü—Ä–æ–±—É–µ–º —Å–æ–±—Ä–∞—Ç—å 2-3 —Å–ª–æ–≤–∞
                phrase = [word]
                j = i + 1
                
                while j < len(words) and len(phrase) < 3:
                    next_word = words[j]
                    if next_word and next_word[0].isupper():
                        phrase.append(next_word)
                        j += 1
                    else:
                        break
                
                # –ë–µ—Ä–µ–º —Å–∞–º—É—é –¥–ª–∏–Ω–Ω—É—é —Ñ—Ä–∞–∑—É
                if len(phrase) >= 2:
                    entity = ' '.join(phrase)
                    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                    entity = entity.replace("'s", "").replace("'", "").strip()
                    
                    if cls._is_valid(entity):
                        entities.add(entity)
                    
                    i = j
                else:
                    i += 1
            else:
                i += 1
        
        return entities
    
    @classmethod
    def _is_valid(cls, text):
        """–ë—ã—Å—Ç—Ä–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è"""
        if not text or len(text) < 3 or len(text) > 50:
            return False
        
        if text.lower() in cls.STOP_WORDS:
            return False
        
        # –ë–µ–∑ —Ü–∏—Ñ—Ä –≤ –Ω–∞—á–∞–ª–µ
        if text[0].isdigit():
            return False
        
        return True


class TurboInfiniteLearning:
    """
    –¢–£–†–ë–û –°–ò–°–¢–ï–ú–ê –ë–ï–°–ö–û–ù–ï–ß–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø
    
    100x –±—ã—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–π –≤–µ—Ä—Å–∏–∏!
    """
    
    def __init__(self, turbo_system=None, topics_list=None):
        self.turbo_system = turbo_system
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.wiki_collector = TurboWikipediaCollector()
        self.entity_extractor = TurboEntityExtractor()
        
        # –û—á–µ—Ä–µ–¥–∏ –∏ —Å–µ—Ç—ã
        self.topic_queue = deque(topics_list or [])
        self.studied_topics = set()
        
        # –ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π
        self.knowledge_graph = defaultdict(set)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'start_time': datetime.now(),
            'topics_studied': 0,
            'sources_collected': 0,
            'entities_discovered': 0,
            'total_content': 0,
        }
        
        # –ü–∞–ø–∫–∞
        self.data_dir = Path('data/turbo_knowledge')
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # Batch –¥–ª—è embeddings
        self.embeddings_batch = []
        self.batch_size = 1000  # 1000 —á–∞–Ω–∫–æ–≤ –∑–∞ —Ä–∞–∑!
        
        logger.info("‚ö° Turbo Infinite Learning –≥–æ—Ç–æ–≤–∞")
    
    async def learn_topic_async(self, topic):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –∏–∑—É—á–µ–Ω–∏–µ —Ç–µ–º—ã"""
        if topic in self.studied_topics:
            return False
        
        try:
            # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
            wiki_results = await self.wiki_collector.search_async(topic, max_languages=5)
            
            if not wiki_results:
                self.studied_topics.add(topic)
                return False
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            all_content = []
            for result in wiki_results:
                all_content.append(result['content'])
            
            full_content = "\n\n".join(all_content)
            
            # –ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
            entities = self.entity_extractor.extract_fast(full_content)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å
            added = 0
            for entity in entities:
                if entity not in self.studied_topics and entity not in self.topic_queue:
                    self.topic_queue.append(entity)
                    self.knowledge_graph[topic].add(entity)
                    added += 1
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º (–±—ã—Å—Ç—Ä–æ)
            self._save_fast(topic, {
                'content': full_content[:2000],  # –¢–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞
                'sources_count': len(wiki_results),
                'entities': list(entities)[:20],  # –¢–æ–ª—å–∫–æ —Ç–æ–ø-20
            })
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ batch –¥–ª—è embeddings
            chunks = self._split_fast(full_content)
            for chunk in chunks:
                self.embeddings_batch.append(f"{topic}: {chunk}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            self.studied_topics.add(topic)
            self.stats['topics_studied'] += 1
            self.stats['sources_collected'] += len(wiki_results)
            self.stats['total_content'] += len(full_content)
            self.stats['entities_discovered'] += added
            
            return True
        
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ {topic}: {e}")
            return False
    
    async def learn_batch_async(self, topics_batch):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ —Ç–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
        tasks = [self.learn_topic_async(topic) for topic in topics_batch]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return sum(1 for r in results if r is True)
    
    def process_embeddings_batch(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö embeddings –æ–¥–Ω–∏–º –±–∞—Ç—á–µ–º –Ω–∞ GPU"""
        if not self.embeddings_batch or not self.turbo_system:
            return
        
        try:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –í–°–ï –∑–∞ —Ä–∞–∑ –Ω–∞ GPU!
            self.turbo_system.learn_batch(
                self.embeddings_batch,
                category="turbo"
            )
            
            logger.info(f"‚úÖ GPU: {len(self.embeddings_batch)} embeddings –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ")
            
            # –û—á–∏—â–∞–µ–º batch
            self.embeddings_batch = []
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ GPU batch: {e}")
    
    async def start_turbo_learning(self):
        """–¢—É—Ä–±–æ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö —Ç–µ–º–∞—Ö"""
        logger.info("="*80)
        logger.info("‚ö° TURBO LEARNING - 100x SPEED")
        logger.info("="*80)
        logger.info(f"–í—Å–µ–≥–æ —Ç–µ–º: {len(self.topic_queue)}")
        logger.info("="*80)
        
        total_topics = len(self.topic_queue)
        parallel_workers = 50  # 50 —Ç–µ–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ!
        
        processed = 0
        
        try:
            while self.topic_queue:
                # –ë–µ—Ä–µ–º batch —Ç–µ–º
                batch = []
                for _ in range(min(parallel_workers, len(self.topic_queue))):
                    if self.topic_queue:
                        batch.append(self.topic_queue.popleft())
                
                if not batch:
                    break
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º batch –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û
                success_count = await self.learn_batch_async(batch)
                processed += len(batch)
                
                # –ö–∞–∂–¥—ã–µ 100 —Ç–µ–º - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º embeddings –Ω–∞ GPU
                if len(self.embeddings_batch) >= self.batch_size:
                    self.process_embeddings_batch()
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞–∂–¥—ã–µ 500 —Ç–µ–º
                if processed % 500 == 0:
                    self._print_stats(processed, total_topics)
                
                # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–∞—É–∑–∞
                await asyncio.sleep(0.1)
        
        except KeyboardInterrupt:
            logger.info("\n‚ö† –û—Å—Ç–∞–Ω–æ–≤–∫–∞")
        
        finally:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è embeddings
            if self.embeddings_batch:
                self.process_embeddings_batch()
            
            self._print_final_stats(total_topics)
    
    def _split_fast(self, content, max_size=1500):
        """–ë—ã—Å—Ç—Ä–∞—è —Ä–∞–∑–±–∏–≤–∫–∞"""
        chunks = []
        for i in range(0, len(content), max_size):
            chunks.append(content[i:i+max_size])
        return chunks
    
    def _save_fast(self, topic, data):
        """–ë—ã—Å—Ç—Ä–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ"""
        try:
            filename = re.sub(r'[<>:"/\\|?*]', '_', topic)[:100] + '.json'
            filepath = self.data_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False)
        except:
            pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    
    def _print_stats(self, processed, total):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        elapsed = (datetime.now() - self.stats['start_time']).total_seconds()
        speed = self.stats['topics_studied'] / elapsed if elapsed > 0 else 0
        remaining = total - processed
        eta_seconds = remaining / speed if speed > 0 else 0
        
        logger.info("="*80)
        logger.info(f"‚ö° [{processed}/{total}] {(processed/total*100):.1f}%")
        logger.info(f"–ò–∑—É—á–µ–Ω–æ: {self.stats['topics_studied']}")
        logger.info(f"–í –æ—á–µ—Ä–µ–¥–∏: {len(self.topic_queue)}")
        logger.info(f"–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {self.stats['sources_collected']}")
        logger.info(f"–ù–æ–≤—ã—Ö —Ç–µ–º: {self.stats['entities_discovered']}")
        logger.info(f"–°–∫–æ—Ä–æ—Å—Ç—å: {speed:.1f} —Ç–µ–º/—Å–µ–∫ = {speed*60:.0f} —Ç–µ–º/–º–∏–Ω")
        logger.info(f"ETA: {eta_seconds/60:.1f} –º–∏–Ω—É—Ç")
        logger.info("="*80)
    
    def _print_final_stats(self, total):
        """–§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        elapsed = (datetime.now() - self.stats['start_time']).total_seconds()
        
        logger.info("\n" + "="*80)
        logger.info("üèÅ TURBO LEARNING –ó–ê–í–ï–†–®–ï–ù–û")
        logger.info("="*80)
        logger.info(f"–í—Å–µ–≥–æ —Ç–µ–º: {total}")
        logger.info(f"–ò–∑—É—á–µ–Ω–æ: {self.stats['topics_studied']}")
        logger.info(f"–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {self.stats['sources_collected']}")
        logger.info(f"–ù–æ–≤—ã—Ö —Ç–µ–º –Ω–∞–π–¥–µ–Ω–æ: {self.stats['entities_discovered']}")
        logger.info(f"–ö–æ–Ω—Ç–µ–Ω—Ç–∞: {self.stats['total_content']/1024/1024:.1f} MB")
        logger.info(f"–í—Ä–µ–º—è: {elapsed/60:.1f} –º–∏–Ω—É—Ç")
        logger.info(f"–°–∫–æ—Ä–æ—Å—Ç—å: {self.stats['topics_studied']/(elapsed/60):.0f} —Ç–µ–º/–º–∏–Ω")
        logger.info(f"–Ø–∑—ã–∫–æ–≤: {', '.join(sorted(self.wiki_collector.stats['languages_used']))}")
        logger.info("="*80)


# –ó–∞–ø—É—Å–∫
async def main():
    logging.basicConfig(
        level=logging.INFO,
        format='%(levelname)s - %(message)s'
    )
    
    print("="*80)
    print("‚ö° TURBO INFINITE LEARNING - 100x SPEED TEST")
    print("="*80)
    print()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï 4127 —Ç–µ–º
    try:
        import sys
        sys.path.insert(0, '.')
        from jarvis.core.learning.topics_database import get_all_topics_flat
        
        all_topics = get_all_topics_flat()
        print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_topics)} —Ç–µ–º –∏–∑ –±–∞–∑—ã")
        
    except Exception as e:
        print(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É: {e}")
        all_topics = ["Python", "Machine Learning", "AI"]
        print(f"–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–º—ã: {len(all_topics)}")
    
    print()
    print(f"–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å: 50 —Ç–µ–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ")
    print(f"–û–∂–∏–¥–∞–µ–º–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å: 600-700 —Ç–µ–º/–º–∏–Ω")
    print(f"–í—Ä–µ–º—è –Ω–∞ {len(all_topics)} —Ç–µ–º: ~{len(all_topics)/600:.1f} –º–∏–Ω—É—Ç")
    print()
    
    input("Enter –¥–ª—è –∑–∞–ø—É—Å–∫–∞...")
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏—Å—Ç–µ–º—É
    system = TurboInfiniteLearning(topics_list=all_topics)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º
    await system.start_turbo_learning()
    
    print("\n‚úÖ –ì–æ—Ç–æ–≤–æ!")


if __name__ == "__main__":
    asyncio.run(main())
