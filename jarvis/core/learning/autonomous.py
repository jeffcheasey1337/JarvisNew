"""
–ú–æ–¥—É–ª—å –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è JARVIS –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
–°–∏—Å—Ç–µ–º–∞ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∏—â–µ—Ç, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ –∏–∑—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
"""

import asyncio
import logging
from datetime import datetime, timedelta
from pathlib import Path
import json
import random
from typing import List, Dict
import feedparser
import requests
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS
import hashlib

logger = logging.getLogger(__name__)


class AutonomousLearning:
    """–ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞"""
    
    def __init__(self, config, memory_system, nlp_processor):
        self.config = config
        self.memory = memory_system
        self.nlp = nlp_processor
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        self.learning_enabled = config.get('autonomous_learning', {}).get('enabled', True)
        self.learning_interval = config.get('autonomous_learning', {}).get('interval_hours', 6)
        self.topics_of_interest = self._load_topics()
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        self.news_sources = self._get_news_sources()
        self.learning_queries = []
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'articles_processed': 0,
            'knowledge_items_learned': 0,
            'last_learning_session': None
        }
        
        logger.info("üåê –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    def _load_topics(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–º –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è"""
        topics_file = Path("data/learning_topics.json")
        
        if topics_file.exists():
            with open(topics_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('topics', [])
        
        # –¢–µ–º—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        default_topics = [
            "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
            "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
            "–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏",
            "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
            "–Ω–∞—É–∫–∞",
            "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ",
            "–∫–æ—Å–º–æ—Å",
            "—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞",
            "–∫–≤–∞–Ω—Ç–æ–≤—ã–µ –∫–æ–º–ø—å—é—Ç–µ—Ä—ã",
            "–±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"
        ]
        
        self._save_topics(default_topics)
        return default_topics
    
    def _save_topics(self, topics: List[str]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–º"""
        topics_file = Path("data/learning_topics.json")
        with open(topics_file, 'w', encoding='utf-8') as f:
            json.dump({'topics': topics}, f, ensure_ascii=False, indent=2)
    
    def _get_news_sources(self) -> List[Dict]:
        """RSS-–ª–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        return [
            {
                'name': 'Habr',
                'url': 'https://habr.com/ru/rss/hub/artificial_intelligence/all/',
                'category': 'AI'
            },
            {
                'name': 'Arxiv AI',
                'url': 'http://export.arxiv.org/rss/cs.AI',
                'category': 'Research'
            },
            {
                'name': 'MIT News AI',
                'url': 'https://news.mit.edu/topic/mitartificial-intelligence2-rss.xml',
                'category': 'Research'
            },
            {
                'name': 'TechCrunch',
                'url': 'https://techcrunch.com/feed/',
                'category': 'Tech'
            }
        ]
    
    async def start_continuous_learning(self):
        """
        –ó–∞–ø—É—Å–∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        –†–∞–±–æ—Ç–∞–µ—Ç –≤ —Ñ–æ–Ω–µ –∏ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –∏–∑—É—á–∞–µ—Ç –Ω–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        """
        if not self.learning_enabled:
            logger.info("‚è∏Ô∏è  –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            return
        
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞")
        logger.info(f"üìä –ò–Ω—Ç–µ—Ä–≤–∞–ª –æ–±—É—á–µ–Ω–∏—è: –∫–∞–∂–¥—ã–µ {self.learning_interval} —á–∞—Å–æ–≤")
        
        while True:
            try:
                # –°–µ—Å—Å–∏—è –æ–±—É—á–µ–Ω–∏—è
                await self.learning_session()
                
                # –ü–∞—É–∑–∞ –¥–æ —Å–ª–µ–¥—É—é—â–µ–π —Å–µ—Å—Å–∏–∏
                wait_seconds = self.learning_interval * 3600
                logger.info(f"üí§ –°–ª–µ–¥—É—é—â–∞—è —Å–µ—Å—Å–∏—è —á–µ—Ä–µ–∑ {self.learning_interval} —á–∞—Å–æ–≤")
                await asyncio.sleep(wait_seconds)
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Å–µ—Å—Å–∏–∏ –æ–±—É—á–µ–Ω–∏—è: {e}")
                await asyncio.sleep(3600)  # –ü–∞—É–∑–∞ —á–∞—Å –ø—Ä–∏ –æ—à–∏–±–∫–µ
    
    async def learning_session(self):
        """–û–¥–Ω–∞ —Å–µ—Å—Å–∏—è –æ–±—É—á–µ–Ω–∏—è"""
        logger.info("=" * 60)
        logger.info("üß† –ù–ê–ß–ê–õ–û –°–ï–°–°–ò–ò –û–ë–£–ß–ï–ù–ò–Ø")
        logger.info("=" * 60)
        
        session_start = datetime.now()
        learned_items = 0
        
        # 1. –û–±—É—á–µ–Ω–∏–µ –∏–∑ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –ª–µ–Ω—Ç
        logger.info("üì∞ –ò–∑—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö RSS-–ª–µ–Ω—Ç...")
        learned_items += await self._learn_from_rss_feeds()
        
        # 2. –ü–æ–∏—Å–∫ –ø–æ –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏–º —Ç–µ–º–∞–º
        logger.info("üîç –ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —Ç–µ–º–∞–º...")
        learned_items += await self._learn_from_search()
        
        # 3. –ò–∑—É—á–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö —Ç–µ–º
        logger.info("üìà –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤...")
        learned_items += await self._learn_trending_topics()
        
        # 4. –£–≥–ª—É–±–ª–µ–Ω–Ω–æ–µ –∏–∑—É—á–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
        logger.info("üìñ –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–µ–π...")
        learned_items += await self._deep_article_analysis()
        
        # 5. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã—É—á–µ–Ω–Ω–æ–≥–æ
        await self._update_interests()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ—Å—Å–∏–∏
        session_duration = (datetime.now() - session_start).total_seconds()
        self.stats['articles_processed'] += learned_items
        self.stats['knowledge_items_learned'] += learned_items
        self.stats['last_learning_session'] = datetime.now().isoformat()
        
        logger.info("=" * 60)
        logger.info(f"‚úÖ –°–ï–°–°–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
        logger.info(f"üìä –ò–∑—É—á–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {learned_items}")
        logger.info(f"‚è±Ô∏è  –í—Ä–µ–º—è: {session_duration:.1f} —Å–µ–∫")
        logger.info(f"üìö –í—Å–µ–≥–æ –∑–Ω–∞–Ω–∏–π: {self.stats['knowledge_items_learned']}")
        logger.info("=" * 60)
        
        self._save_stats()
    
    async def _learn_from_rss_feeds(self) -> int:
        """–û–±—É—á–µ–Ω–∏–µ –∏–∑ RSS-–ª–µ–Ω—Ç"""
        learned = 0
        
        for source in self.news_sources:
            try:
                logger.info(f"  üì° –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ {source['name']}...")
                
                # –ü–∞—Ä—Å–∏–Ω–≥ RSS
                feed = feedparser.parse(source['url'])
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 5 —Å—Ç–∞—Ç–µ–π
                for entry in feed.entries[:5]:
                    title = entry.get('title', '')
                    summary = entry.get('summary', entry.get('description', ''))
                    link = entry.get('link', '')
                    
                    if not title:
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω–µ –∏–∑—É—á–∞–ª–∏ –ª–∏ —É–∂–µ
                    if await self._already_learned(link):
                        continue
                    
                    # –°–æ–∑–¥–∞–Ω–∏–µ –∑–Ω–∞–Ω–∏—è
                    knowledge = f"–°—Ç–∞—Ç—å—è: {title}. {summary}"
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ø–∞–º—è—Ç—å
                    await self.memory.store_memory(
                        knowledge,
                        memory_type="learned_knowledge",
                        metadata={
                            'source': source['name'],
                            'category': source['category'],
                            'url': link,
                            'learned_at': datetime.now().isoformat()
                        }
                    )
                    
                    learned += 1
                    logger.info(f"    ‚úÖ –ò–∑—É—á–µ–Ω–æ: {title[:60]}...")
                
            except Exception as e:
                logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ —Å {source['name']}: {e}")
        
        return learned
    
    async def _learn_from_search(self) -> int:
        """–ê–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —Ç–µ–º–∞–º"""
        learned = 0
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —Ç–µ–º—ã –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è
        topics_to_learn = random.sample(
            self.topics_of_interest, 
            min(3, len(self.topics_of_interest))
        )
        
        for topic in topics_to_learn:
            try:
                logger.info(f"  üîç –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–º–µ: {topic}")
                
                # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo
                with DDGS() as ddgs:
                    results = list(ddgs.text(
                        f"{topic} –Ω–æ–≤–æ—Å—Ç–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è 2024 2025",
                        max_results=3
                    ))
                
                for result in results:
                    title = result.get('title', '')
                    body = result.get('body', '')
                    url = result.get('href', '')
                    
                    if await self._already_learned(url):
                        continue
                    
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ NLP
                    knowledge = f"–¢–µ–º–∞ '{topic}': {title}. {body}"
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                    await self.memory.store_memory(
                        knowledge,
                        memory_type="learned_knowledge",
                        metadata={
                            'source': 'web_search',
                            'topic': topic,
                            'url': url,
                            'learned_at': datetime.now().isoformat(),
                            'importance': 0.7
                        }
                    )
                    
                    learned += 1
                    logger.info(f"    ‚úÖ {title[:50]}...")
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ '{topic}': {e}")
        
        return learned
    
    async def _learn_trending_topics(self) -> int:
        """–ò–∑—É—á–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö —Ç–µ–º"""
        learned = 0
        
        try:
            # –ü–æ–∏—Å–∫ —Ç—Ä–µ–Ω–¥–æ–≤
            trending_queries = [
                "AI –Ω–æ–≤–æ—Å—Ç–∏ —Å–µ–≥–æ–¥–Ω—è",
                "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ—Ä—ã–≤—ã 2025",
                "–ø–æ—Å–ª–µ–¥–Ω–∏–µ –æ—Ç–∫—Ä—ã—Ç–∏—è –≤ –Ω–∞—É–∫–µ"
            ]
            
            query = random.choice(trending_queries)
            logger.info(f"  üìà –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤: {query}")
            
            with DDGS() as ddgs:
                results = list(ddgs.text(query, max_results=5))
            
            for result in results:
                if await self._already_learned(result.get('href', '')):
                    continue
                
                knowledge = f"–¢—Ä–µ–Ω–¥: {result.get('title', '')}. {result.get('body', '')}"
                
                await self.memory.store_memory(
                    knowledge,
                    memory_type="learned_knowledge",
                    metadata={
                        'source': 'trending',
                        'url': result.get('href', ''),
                        'learned_at': datetime.now().isoformat(),
                        'importance': 0.8
                    }
                )
                
                learned += 1
            
        except Exception as e:
            logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
        
        return learned
    
    async def _deep_article_analysis(self) -> int:
        """–£–≥–ª—É–±–ª–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç–µ–π"""
        learned = 0
        
        # –í—ã–±–∏—Ä–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ URL –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è
        try:
            # –ü–æ–ª—É—á–∞–µ–º –Ω–µ–¥–∞–≤–Ω–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
            recent_articles = await self._get_recent_urls(limit=2)
            
            for url in recent_articles:
                try:
                    logger.info(f"  üìñ –î–µ—Ç–∞–ª—å–Ω–æ–µ –∏–∑—É—á–µ–Ω–∏–µ: {url[:50]}...")
                    
                    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    response = requests.get(url, timeout=10)
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                    paragraphs = soup.find_all('p')
                    text = ' '.join([p.get_text() for p in paragraphs[:10]])
                    
                    if len(text) < 100:
                        continue
                    
                    # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ NLP
                    summary = await self.nlp.summarize_text(text, max_length=200)
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞–Ω–∏—è
                    await self.memory.store_memory(
                        f"–î–µ—Ç–∞–ª—å–Ω–æ–µ –∏–∑—É—á–µ–Ω–∏–µ: {summary}",
                        memory_type="deep_knowledge",
                        metadata={
                            'source': 'deep_analysis',
                            'url': url,
                            'learned_at': datetime.now().isoformat(),
                            'importance': 0.9
                        }
                    )
                    
                    learned += 1
                    await asyncio.sleep(3)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    
                except Exception as e:
                    logger.error(f"    ‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {url}: {e}")
            
        except Exception as e:
            logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        
        return learned
    
    async def _already_learned(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω–µ –∏–∑—É—á–∞–ª–∏ –ª–∏ —É–∂–µ —ç—Ç–æ—Ç URL"""
        if not url:
            return True
        
        # –°–æ–∑–¥–∞–µ–º —Ö—ç—à URL –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
        url_hash = hashlib.md5(url.encode()).hexdigest()
        
        # –ò—â–µ–º –≤ –ø–∞–º—è—Ç–∏
        try:
            results = await self.memory.recall_memory(url_hash, n_results=1)
            return len(results) > 0
        except:
            return False
    
    async def _get_recent_urls(self, limit: int = 5) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–µ–¥–∞–≤–Ω–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö URL –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è"""
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤—ã–±–æ—Ä–∫—É –∏–∑ –±–∞–∑—ã –ø–∞–º—è—Ç–∏
        # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
        return []
    
    async def _update_interests(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–µ–º –∏–Ω—Ç–µ—Ä–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã—É—á–µ–Ω–Ω–æ–≥–æ"""
        try:
            # –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è —Ç–µ–º
            # –ú–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å —Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –ø–æ—è–≤–ª—è—é—Ç—Å—è
            
            logger.info("  üéØ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤...")
            
            # –ü—Ä–∏–º–µ—Ä: –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö —Ç–µ–º
            new_topics = []
            
            # –ü–æ–∏—Å–∫ –Ω–æ–≤—ã—Ö –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö —Ç–µ–º —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –≤—ã—É—á–µ–Ω–Ω–æ–≥–æ
            # (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
            
            if new_topics:
                self.topics_of_interest.extend(new_topics)
                self.topics_of_interest = list(set(self.topics_of_interest))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
                self._save_topics(self.topics_of_interest)
                logger.info(f"    ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö —Ç–µ–º: {len(new_topics)}")
            
        except Exception as e:
            logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤: {e}")
    
    def _save_stats(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        stats_file = Path("data/learning_stats.json")
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
    
    async def add_topic(self, topic: str):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–π —Ç–µ–º—ã –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è"""
        if topic not in self.topics_of_interest:
            self.topics_of_interest.append(topic)
            self._save_topics(self.topics_of_interest)
            logger.info(f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Ç–µ–º–∞ –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è: {topic}")
    
    async def remove_topic(self, topic: str):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Ç–µ–º—ã –∏–∑ –∏–∑—É—á–µ–Ω–∏—è"""
        if topic in self.topics_of_interest:
            self.topics_of_interest.remove(topic)
            self._save_topics(self.topics_of_interest)
            logger.info(f"‚ûñ –¢–µ–º–∞ —É–¥–∞–ª–µ–Ω–∞ –∏–∑ –∏–∑—É—á–µ–Ω–∏—è: {topic}")
    
    async def get_learning_report(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –æ–±—É—á–µ–Ω–∏–∏"""
        return {
            'total_articles': self.stats['articles_processed'],
            'total_knowledge': self.stats['knowledge_items_learned'],
            'last_session': self.stats['last_learning_session'],
            'topics': self.topics_of_interest,
            'sources': len(self.news_sources)
        }
    
    async def manual_learning_session(self, topic: str = None):
        """–†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫ —Å–µ—Å—Å–∏–∏ –æ–±—É—á–µ–Ω–∏—è –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç–µ–º–µ"""
        if topic:
            logger.info(f"üéØ –†—É—á–Ω–∞—è —Å–µ—Å—Å–∏—è –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–µ–º–µ: {topic}")
            # –í—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–º–µ–Ω—è–µ–º —Ç–µ–º—ã
            original_topics = self.topics_of_interest.copy()
            self.topics_of_interest = [topic]
            
            await self.learning_session()
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–º—ã
            self.topics_of_interest = original_topics
        else:
            await self.learning_session()
