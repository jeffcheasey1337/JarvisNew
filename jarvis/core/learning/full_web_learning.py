# -*- coding: utf-8 -*-
"""
üåê FULL WEB CRAWLER - –í–ï–°–¨ –ò–ù–¢–ï–†–ù–ï–¢!
–ö—Ä–∞—É–ª–µ—Ä –∫–æ—Ç–æ—Ä—ã–π —Å–æ–±–∏—Ä–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –õ–Æ–ë–´–• –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

–ò—Å—Ç–æ—á–Ω–∏–∫–∏:
‚úÖ DuckDuckGo (—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Google)
‚úÖ –õ—é–±—ã–µ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã
‚úÖ –ù–æ–≤–æ—Å—Ç–Ω—ã–µ —Å–∞–π—Ç—ã
‚úÖ –ë–ª–æ–≥–∏
‚úÖ –§–æ—Ä—É–º—ã
‚úÖ Wikipedia (–∫–∞–∫ –æ–¥–∏–Ω –∏–∑ –º–Ω–æ–≥–∏—Ö)
‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ª—é–±—ã—Ö —Å–∞–π—Ç–æ–≤
"""

import logging
import requests
from bs4 import BeautifulSoup
import time
import re
from pathlib import Path
import json
from collections import defaultdict, deque
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from urllib.parse import urlparse
import hashlib

logger = logging.getLogger(__name__)


class UniversalWebCrawler:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫—Ä–∞—É–ª–µ—Ä - –∏—â–µ—Ç –í–ï–ó–î–ï"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        adapter = HTTPAdapter(
            pool_connections=50,
            pool_maxsize=50,
            max_retries=Retry(total=2, backoff_factor=0.3)
        )
        
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
        
        self.visited_urls = set()
        self.lock = threading.Lock()
        
        self.stats = {
            'pages_crawled': 0,
            'sources_used': set(),
        }
    
    def search_everywhere(self, query, max_results=5):
        """–ü–æ–∏—Å–∫ –í–ï–ó–î–ï"""
        all_results = []
        
        # DuckDuckGo
        ddg_results = self._search_duckduckgo(query, limit=max_results)
        all_results.extend(ddg_results)
        
        # Wikipedia –±—ã—Å—Ç—Ä–æ
        if len(all_results) < 2:
            wiki_results = self._search_wikipedia(query)
            all_results.extend(wiki_results)
        
        # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        parsed_results = []
        for result in all_results[:max_results]:
            content = self._scrape_page(result['url'])
            if content:
                result['content'] = content
                parsed_results.append(result)
        
        with self.lock:
            self.stats['pages_crawled'] += len(parsed_results)
            for r in parsed_results:
                domain = urlparse(r['url']).netloc
                self.stats['sources_used'].add(domain)
        
        return parsed_results
    
    def _search_duckduckgo(self, query, limit=5):
        """DuckDuckGo HTML –ø–æ–∏—Å–∫"""
        results = []
        
        try:
            url = "https://html.duckduckgo.com/html/"
            response = self.session.post(url, data={'q': query}, timeout=5)  # –°–û–ö–†–ê–¢–ò–õ–ò –¢–ê–ô–ú–ê–£–¢!
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                result_divs = soup.find_all('div', class_='result', limit=limit)
                
                for div in result_divs:
                    try:
                        link_tag = div.find('a', class_='result__a')
                        if not link_tag:
                            continue
                        
                        url = link_tag.get('href', '')
                        if not url.startswith('http'):
                            continue
                        
                        title = link_tag.get_text(strip=True)
                        snippet_tag = div.find('a', class_='result__snippet')
                        snippet = snippet_tag.get_text(strip=True) if snippet_tag else ''
                        
                        results.append({
                            'url': url,
                            'title': title,
                            'content': snippet,
                            'source': 'DuckDuckGo'
                        })
                    except:
                        continue
        except Exception as e:
            logger.debug(f"DuckDuckGo –æ—à–∏–±–∫–∞: {e}")
        
        return results
    
    def _search_wikipedia(self, query):
        """Wikipedia –±—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫"""
        results = []
        
        for lang in ['ru', 'en']:
            try:
                api_url = f"https://{lang}.wikipedia.org/w/api.php"
                
                response = self.session.get(api_url, params={
                    'action': 'opensearch',
                    'search': query,
                    'limit': 1,
                    'format': 'json'
                }, timeout=4)  # –°–û–ö–†–ê–¢–ò–õ–ò –¢–ê–ô–ú–ê–£–¢!
                
                if response.status_code != 200:
                    continue
                
                data = response.json()
                if len(data) < 2 or not data[1]:
                    continue
                
                title = data[1][0]
                url = data[3][0] if len(data) > 3 else f"https://{lang}.wikipedia.org/wiki/{title}"
                
                # –ö–æ–Ω—Ç–µ–Ω—Ç
                response = self.session.get(api_url, params={
                    'action': 'query',
                    'prop': 'extracts',
                    'exintro': True,
                    'explaintext': True,
                    'titles': title,
                    'format': 'json'
                }, timeout=8)
                
                if response.status_code == 200:
                    content_data = response.json()
                    pages = content_data.get('query', {}).get('pages', {})
                    
                    for page_data in pages.values():
                        extract = page_data.get('extract', '')
                        if extract and len(extract) > 100:
                            results.append({
                                'url': url,
                                'title': title,
                                'content': extract[:3000],
                                'source': f'Wikipedia ({lang})'
                            })
                            break
            except:
                continue
        
        return results
    
    def _scrape_page(self, url):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            url_hash = hashlib.md5(url.encode()).hexdigest()
            
            with self.lock:
                if url_hash in self.visited_urls:
                    return None
                self.visited_urls.add(url_hash)
            
            response = self.session.get(url, timeout=10)
            
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –£–¥–∞–ª—è–µ–º –º—É—Å–æ—Ä
            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 
                            'iframe', 'noscript', 'form', 'button']):
                tag.decompose()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            content_tag = (
                soup.find('article') or 
                soup.find('main') or 
                soup.find('div', class_=re.compile(r'content|article|post', re.I)) or
                soup.find('body')
            )
            
            if not content_tag:
                return None
            
            text = content_tag.get_text(separator=' ', strip=True)
            text = re.sub(r'\s+', ' ', text)
            
            if len(text) < 100:
                return None
            
            return text[:5000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º
        
        except Exception as e:
            logger.debug(f"–ü–∞—Ä—Å–∏–Ω–≥ {url}: {e}")
            return None


class FastEntityExtractor:
    """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    
    @classmethod
    def extract_fast(cls, text):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ"""
        entities = set()
        words = text.split()
        i = 0
        
        while i < len(words):
            word = words[i]
            
            if word and len(word) > 2 and word[0].isupper():
                phrase = [word]
                j = i + 1
                
                while j < len(words) and len(phrase) < 3:
                    next_word = words[j]
                    if next_word and next_word[0].isupper():
                        phrase.append(next_word)
                        j += 1
                    else:
                        break
                
                if len(phrase) >= 2:
                    entity = ' '.join(phrase).replace("'s", "").strip()
                    if 3 <= len(entity) <= 50:
                        entities.add(entity)
                    i = j
                else:
                    i += 1
            else:
                i += 1
        
        return entities


class FullWebLearningSystem:
    """–ü–û–õ–ù–ê–Ø –í–ï–ë-–°–ò–°–¢–ï–ú–ê –û–ë–£–ß–ï–ù–ò–Ø"""
    
    def __init__(self, turbo_system=None, memory_system=None, topics_list=None, num_workers=10):
        self.turbo_system = turbo_system
        self.memory_system = memory_system
        self.num_workers = num_workers
        
        self.crawler = UniversalWebCrawler()
        self.entity_extractor = FastEntityExtractor()
        
        self.topic_queue = deque(topics_list or [])
        self.studied_topics = set()
        self.knowledge_graph = defaultdict(set)
        
        self.stats = {
            'start_time': datetime.now(),
            'topics_studied': 0,
            'pages_crawled': 0,
            'sources_collected': 0,
            'entities_discovered': 0,
            'total_content': 0,
            'memory_records_added': 0,
        }
        
        self.data_dir = Path('data/web_knowledge')
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        self.embeddings_batch = []
        self.batch_size = 300
        self.lock = threading.Lock()
        
        # Dashboard support
        self.dashboard = None
        self.current_thread_topics = {}  # thread_id -> current_topic
        
        logger.info(f"Full Web Learning –≥–æ—Ç–æ–≤–∞ ({num_workers} –ø–æ—Ç–æ–∫–æ–≤)")
    
    def enable_dashboard(self):
        """–í–∫–ª—é—á–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ dashboard"""
        try:
            from .learning_dashboard import LearningDashboard
            self.dashboard = LearningDashboard(self)
            self.dashboard.start()
            logger.info("‚úÖ Dashboard –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω!")
        except ImportError:
            logger.warning("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –º–æ–¥—É–ª—å learning_dashboard")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ dashboard: {e}")
    
    def learn_topic(self, topic, thread_id=None):
        """–ò–∑—É—á–µ–Ω–∏–µ —Ç–µ–º—ã –∏–∑ –≤—Å–µ–≥–æ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞"""
        if topic in self.studied_topics:
            return False
        
        # Dashboard update
        if self.dashboard and thread_id is not None:
            self.dashboard.update_thread_status(thread_id, topic, 'searching')
        
        logger.info(f"üîç –ù–∞—á–∏–Ω–∞—é –∏–∑—É—á–µ–Ω–∏–µ: {topic}")
        
        try:
            # –ë–´–°–¢–†–´–ô –ü–û–ò–°–ö: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ–º—ã –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–≥–æ –∏—â—É—Ç—Å—è
            results = []
            search_success = [False]
            
            def do_search():
                try:
                    search_success[0] = True
                    return self.crawler.search_everywhere(topic, max_results=5)
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ {topic}: {e}")
                    return []
            
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å –∫–æ—Ä–æ—Ç–∫–∏–º timeout
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(do_search)
                try:
                    results = future.result(timeout=20)  # 20 —Å–µ–∫—É–Ω–¥ –º–∞–∫—Å–∏–º—É–º
                except concurrent.futures.TimeoutError:
                    logger.warning(f"‚è±Ô∏è –¢–∞–π–º–∞—É—Ç –ø–æ–∏—Å–∫–∞: {topic}")
                    with self.lock:
                        self.studied_topics.add(topic)
                    return False
            
            if not results:
                with self.lock:
                    self.studied_topics.add(topic)
                logger.debug(f"‚ùå {topic}: –Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                return False
            
            logger.info(f"‚úì {topic}: –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            
            # Dashboard update - parsing
            if self.dashboard and thread_id is not None:
                self.dashboard.update_thread_status(thread_id, topic, 'parsing')
            
            all_content = []
            for result in results:
                if result.get('content'):
                    all_content.append(result['content'])
            
            if not all_content:
                with self.lock:
                    self.studied_topics.add(topic)
                return False
            
            full_content = "\n\n".join(all_content)
            
            # –°—É—â–Ω–æ—Å—Ç–∏
            entities = self.entity_extractor.extract_fast(full_content)
            
            added = 0
            with self.lock:
                for entity in entities:
                    if entity not in self.studied_topics and entity not in self.topic_queue:
                        self.topic_queue.append(entity)
                        self.knowledge_graph[topic].add(entity)
                        added += 1
            
            # Dashboard update - saving to memory
            if self.dashboard and thread_id is not None:
                self.dashboard.update_thread_status(thread_id, topic, 'saving')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å - BATCH –º–µ—Ç–æ–¥ —Å –º–∞—Å—Å–æ–≤—ã–º –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º!
            memory_added = 0
            if self.memory_system:
                logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å {topic} –≤ –ø–∞–º—è—Ç—å...")
                try:
                    chunks = self._split_content(full_content, max_size=400)
                    logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –¥–ª—è {topic}")
                    
                    # –¢–£–†–ë–û –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: Batch –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –í–°–ï–• —á–∞–Ω–∫–æ–≤ —Å—Ä–∞–∑—É!
                    if chunks[:5]:
                        import datetime
                        
                        # –ú–ï–¢–û–î 1: –ü—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ ChromaDB (–±—ã—Å—Ç—Ä–æ)
                        try:
                            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ batch –¥–∞–Ω–Ω—ã—Ö
                            batch_embeddings = []
                            batch_documents = []
                            batch_metadatas = []
                            batch_ids = []
                            
                            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –í–°–ï–• —á–∞–Ω–∫–æ–≤ –°–†–ê–ó–£ (–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è!)
                            texts = [f"{topic}: {chunk}" for chunk in chunks[:5]]
                            batch_embeddings = self.memory_system.embedder.encode(texts).tolist()
                            
                            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                            base_timestamp = datetime.datetime.now().timestamp()
                            for idx, (chunk, embedding) in enumerate(zip(chunks[:5], batch_embeddings)):
                                batch_documents.append(f"{topic}: {chunk}")
                                batch_metadatas.append({
                                    'type': 'knowledge',
                                    'timestamp': datetime.datetime.now().isoformat(),
                                    'importance': 0.7,
                                    'topic': topic,
                                    'source': 'web_crawler',
                                    'auto_learned': True
                                })
                                batch_ids.append(f"knowledge_{base_timestamp}_{idx}")
                            
                            # –ú–ê–°–°–û–í–û–ï –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–¥–Ω–∏–º –≤—ã–∑–æ–≤–æ–º!
                            self.memory_system.collection.add(
                                embeddings=batch_embeddings,
                                documents=batch_documents,
                                metadatas=batch_metadatas,
                                ids=batch_ids
                            )
                            
                            memory_added = len(batch_documents)
                            logger.info(f"‚úÖ Batch —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {memory_added} –∑–∞–ø–∏—Å–µ–π")
                            
                        except Exception as batch_error:
                            logger.warning(f"Batch –º–µ—Ç–æ–¥ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {batch_error}")
                            logger.info("–ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥...")
                            
                            # –ú–ï–¢–û–î 2: FALLBACK - –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ asyncio (–º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ –Ω–∞–¥—ë–∂–Ω–µ–µ)
                            import asyncio
                            for chunk in chunks[:5]:
                                try:
                                    asyncio.run(self.memory_system.store_memory(
                                        content=f"{topic}: {chunk}",
                                        memory_type="knowledge",
                                        metadata={
                                            'topic': topic,
                                            'source': 'web_crawler',
                                            'auto_learned': True
                                        }
                                    ))
                                    memory_added += 1
                                except Exception as e:
                                    logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–∞–Ω–∫–∞: {e}")
                            
                            logger.info(f"‚úÖ Async —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {memory_added} –∑–∞–ø–∏—Å–µ–π")
                        
                        with self.lock:
                            self.stats['memory_records_added'] += memory_added
                        
                        logger.info(f"–í –ø–∞–º—è—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–æ {memory_added} –∑–∞–ø–∏—Å–µ–π –¥–ª—è {topic}")
                
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è {topic}: {e}", exc_info=True)
            else:
                logger.warning("memory_system = None! –ù–µ –º–æ–≥—É —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ø–∞–º—è—Ç—å!")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            self._save_fast(topic, {
                'content': full_content[:2000],
                'sources': [{'url': r['url'], 'source': r['source']} for r in results],
                'entities': list(entities)[:20],
            })
            
            # Embeddings
            chunks = self._split_content(full_content)
            with self.lock:
                for chunk in chunks:
                    self.embeddings_batch.append(f"{topic}: {chunk}")
            
            with self.lock:
                self.studied_topics.add(topic)
                self.stats['topics_studied'] += 1
                self.stats['sources_collected'] += len(results)
                self.stats['pages_crawled'] = self.crawler.stats['pages_crawled']
                self.stats['total_content'] += len(full_content)
                self.stats['entities_discovered'] += added
            
            # Dashboard update - completed
            if self.dashboard and thread_id is not None:
                self.dashboard.update_thread_status(thread_id, topic, 'completed')
            
            return True
        
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ {topic}: {e}")
            
            # Dashboard update - error
            if self.dashboard and thread_id is not None:
                self.dashboard.update_thread_status(thread_id, topic, 'error')
            
            return False
    
    def process_embeddings_batch(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ embeddings"""
        with self.lock:
            if not self.embeddings_batch or not self.turbo_system:
                return
            
            batch = self.embeddings_batch.copy()
            self.embeddings_batch = []
        
        try:
            self.turbo_system.learn_batch(batch, category="web")
            logger.info(f"GPU: {len(batch)} embeddings")
        except Exception as e:
            logger.debug(f"GPU –æ—à–∏–±–∫–∞: {e}")
    
    def start_web_learning(self):
        """–ó–∞–ø—É—Å–∫ –≤–µ–±-–æ–±—É—á–µ–Ω–∏—è"""
        logger.info("="*80)
        logger.info(f"FULL WEB LEARNING - {self.num_workers} –ü–û–¢–û–ö–û–í")
        logger.info("="*80)
        logger.info(f"–í—Å–µ–≥–æ —Ç–µ–º: {len(self.topic_queue)}")
        logger.info("="*80)
        
        total_topics = len(self.topic_queue)
        processed = 0
        
        try:
            thread_id_counter = [0]  # Mutable counter for thread IDs
            thread_id_map = {}  # Map threads to IDs
            
            with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
                while self.topic_queue:
                    batch = []
                    for _ in range(min(self.num_workers * 2, len(self.topic_queue))):
                        if self.topic_queue:
                            batch.append(self.topic_queue.popleft())
                    
                    if not batch:
                        break
                    
                    # Create futures with thread ID tracking
                    futures = {}
                    for topic in batch:
                        # Assign thread ID
                        thread_id = thread_id_counter[0] % self.num_workers
                        thread_id_counter[0] += 1
                        
                        future = executor.submit(self.learn_topic, topic, thread_id)
                        futures[future] = (topic, thread_id)
                    
                    for future in as_completed(futures):
                        processed += 1
                        
                        if len(self.embeddings_batch) >= self.batch_size:
                            self.process_embeddings_batch()
                        
                        if processed % 50 == 0:
                            self._print_stats(processed, total_topics)
        
        except KeyboardInterrupt:
            logger.info("\n–û—Å—Ç–∞–Ω–æ–≤–∫–∞")
        
        finally:
            if self.embeddings_batch:
                self.process_embeddings_batch()
            
            self._print_final_stats(total_topics)
    
    # –ê–ª–∏–∞—Å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    def start_learning(self):
        """–ê–ª–∏–∞—Å –¥–ª—è start_web_learning()"""
        return self.start_web_learning()
    
    def _split_content(self, content, max_size=1500):
        chunks = []
        for i in range(0, len(content), max_size):
            chunks.append(content[i:i+max_size])
        return chunks
    
    def _save_fast(self, topic, data):
        try:
            filename = re.sub(r'[<>:"/\\|?*]', '_', topic)[:100] + '.json'
            filepath = self.data_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False)
        except:
            pass
    
    def _print_stats(self, processed, total):
        elapsed = (datetime.now() - self.stats['start_time']).total_seconds()
        speed = self.stats['topics_studied'] / elapsed if elapsed > 0 else 0
        eta = (total - processed) / speed if speed > 0 else 0
        
        logger.info("="*80)
        logger.info(f"[{processed}/{total}] {(processed/total*100):.1f}%")
        logger.info(f"–ò–∑—É—á–µ–Ω–æ: {self.stats['topics_studied']}")
        logger.info(f"–í –ü–ê–ú–Ø–¢–¨: +{self.stats['memory_records_added']} –∑–∞–ø–∏—Å–µ–π")
        logger.info(f"–°—Ç—Ä–∞–Ω–∏—Ü: {self.stats['pages_crawled']}")
        logger.info(f"–î–æ–º–µ–Ω–æ–≤: {len(self.crawler.stats['sources_used'])}")
        logger.info(f"–ù–æ–≤—ã—Ö —Ç–µ–º: {self.stats['entities_discovered']}")
        logger.info(f"–°–∫–æ—Ä–æ—Å—Ç—å: {speed*60:.0f} —Ç–µ–º/–º–∏–Ω")
        logger.info(f"ETA: {eta/60:.1f} –º–∏–Ω")
        logger.info("="*80)
    
    def _print_final_stats(self, total):
        elapsed = (datetime.now() - self.stats['start_time']).total_seconds()
        
        logger.info("\n" + "="*80)
        logger.info("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
        logger.info("="*80)
        logger.info(f"–ò–∑—É—á–µ–Ω–æ: {self.stats['topics_studied']}")
        logger.info(f"–í –ü–ê–ú–Ø–¢–¨ –î–û–ë–ê–í–õ–ï–ù–û: {self.stats['memory_records_added']} –∑–∞–ø–∏—Å–µ–π!")
        logger.info(f"–°—Ç—Ä–∞–Ω–∏—Ü: {self.stats['pages_crawled']}")
        logger.info(f"–î–æ–º–µ–Ω–æ–≤: {len(self.crawler.stats['sources_used'])}")
        logger.info(f"–ö–æ–Ω—Ç–µ–Ω—Ç–∞: {self.stats['total_content']/1024/1024:.1f} MB")
        logger.info(f"–í—Ä–µ–º—è: {elapsed/60:.1f} –º–∏–Ω")
        logger.info(f"–°–∫–æ—Ä–æ—Å—Ç—å: {self.stats['topics_studied']/(elapsed/60):.0f} —Ç–µ–º/–º–∏–Ω")
        logger.info("\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–º–µ–Ω—ã:")
        for domain in sorted(self.crawler.stats['sources_used'])[:20]:
            logger.info(f"  - {domain}")
        logger.info("="*80)
