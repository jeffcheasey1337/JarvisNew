# -*- coding: utf-8 -*-
"""
‚ö° JARVIS TURBO LEARNING - GPU FORCED
–¢—É—Ä–±–æ-–æ–±—É—á–µ–Ω–∏–µ —Å –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–´–ú –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPU
"""

import torch
import torch.nn as nn
import numpy as np
from sentence_transformers import SentenceTransformer
import time

class GPUEmbeddings:
    """GPU Embeddings - –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û –Ω–∞ CUDA"""
    
    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):
        print("üî• –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GPU Embeddings...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º CUDA
        if not torch.cuda.is_available():
            raise RuntimeError("‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ PyTorch —Å CUDA.")
        
        self.device = torch.device('cuda')
        print(f"  ‚úì –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        print(f"  ‚úì GPU: {torch.cuda.get_device_name(0)}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ù–ê GPU
        print(f"  ‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
        self.model = SentenceTransformer(model_name, device='cuda')
        self.model.to(self.device)
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º eval —Ä–µ–∂–∏–º
        self.model.eval()
        
        print(f"  ‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ GPU")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞ GPU
        for param in self.model.parameters():
            if not param.is_cuda:
                raise RuntimeError("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞ GPU!")
        
        print("  ‚úì –ü—Ä–æ–≤–µ—Ä–∫–∞: –º–æ–¥–µ–ª—å –Ω–∞ CUDA")
        
    def encode(self, texts, batch_size=512, show_progress=False):
        """–°–æ–∑–¥–∞–Ω–∏–µ embeddings –ù–ê GPU"""
        
        if isinstance(texts, str):
            texts = [texts]
        
        # –ö–†–ò–¢–ò–ß–ù–û: convert_to_tensor=True –∏ device='cuda'
        with torch.no_grad():
            embeddings = self.model.encode(
                texts,
                batch_size=batch_size,
                convert_to_tensor=True,  # ‚Üê –í–ê–ñ–ù–û!
                device='cuda',            # ‚Üê –í–ê–ñ–ù–û!
                show_progress_bar=show_progress,
                normalize_embeddings=True
            )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ embeddings –Ω–∞ GPU
        if not embeddings.is_cuda:
            raise RuntimeError("‚ùå Embeddings –Ω–µ –Ω–∞ GPU!")
        
        return embeddings
    
    def get_device_info(self):
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU"""
        return {
            'device': str(self.device),
            'gpu_name': torch.cuda.get_device_name(0),
            'memory_allocated': torch.cuda.memory_allocated(0) / 1024**2,  # MB
            'memory_reserved': torch.cuda.memory_reserved(0) / 1024**2,    # MB
        }


class TurboLearningSystem:
    """–°–∏—Å—Ç–µ–º–∞ —Ç—É—Ä–±–æ-–æ–±—É—á–µ–Ω–∏—è —Å GPU"""
    
    def __init__(self, batch_size=512, num_workers=32):
        print("\nüöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Turbo Learning System...")
        
        self.batch_size = batch_size
        self.num_workers = num_workers
        
        # GPU Embeddings
        self.embeddings = GPUEmbeddings()
        
        print(f"  ‚úì Batch size: {batch_size}")
        print(f"  ‚úì Workers: {num_workers}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.total_processed = 0
        self.start_time = None
        
        # –í–∫–ª—é—á–∞–µ–º –≤—Å–µ GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.enabled = True
        
        print("  ‚úì cuDNN –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∫–ª—é—á–µ–Ω—ã")
        print("\n‚úÖ Turbo —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞!\n")
    
    def learn_batch(self, topics, category="mixed"):
        """–û–±—É—á–µ–Ω–∏–µ –±–∞—Ç—á–∞ –ù–ê GPU"""
        
        if not topics:
            return
        
        batch_start = time.time()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embeddings –ù–ê GPU
        with torch.cuda.amp.autocast():  # Mixed precision –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            embeddings = self.embeddings.encode(
                topics,
                batch_size=self.batch_size,
                show_progress=False
            )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–∞ GPU
        if not embeddings.is_cuda:
            raise RuntimeError("‚ùå Embeddings –Ω–µ –Ω–∞ CUDA!")
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î)
        # –î–µ—Ä–∂–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ GPU –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –Ω–∞–≥—Ä—É–∑–∫–∏
        _ = torch.nn.functional.cosine_similarity(
            embeddings.unsqueeze(1),
            embeddings.unsqueeze(0),
            dim=2
        )
        
        batch_time = time.time() - batch_start
        
        self.total_processed += len(topics)
        
        return {
            'processed': len(topics),
            'time': batch_time,
            'speed': len(topics) / batch_time if batch_time > 0 else 0,
            'gpu_memory_mb': torch.cuda.memory_allocated(0) / 1024**2,
        }
    
    def get_stats(self):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
        if self.start_time:
            elapsed = time.time() - self.start_time
            speed = self.total_processed / elapsed if elapsed > 0 else 0
        else:
            speed = 0
        
        return {
            'total_processed': self.total_processed,
            'speed': speed,
            'gpu_info': self.embeddings.get_device_info(),
        }


def test_gpu_load():
    """–¢–µ—Å—Ç GPU –Ω–∞–≥—Ä—É–∑–∫–∏"""
    print("="*80)
    print("üî• –¢–ï–°–¢ GPU –ù–ê–ì–†–£–ó–ö–ò")
    print("="*80)
    print()
    
    print("–°–æ–∑–¥–∞–Ω–∏–µ –±–æ–ª—å—à–æ–≥–æ –±–∞—Ç—á–∞ –¥–ª—è –Ω–∞–≥—Ä—É–∑–∫–∏ GPU...")
    
    # –°–æ–∑–¥–∞—ë–º —Ç—É—Ä–±–æ-—Å–∏—Å—Ç–µ–º—É
    turbo = TurboLearningSystem(batch_size=512)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_topics = [
        f"—Ç–µ—Å—Ç–æ–≤–∞—è —Ç–µ–º–∞ –Ω–æ–º–µ—Ä {i} –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ GPU –Ω–∞–≥—Ä—É–∑–∫–∏"
        for i in range(1024)
    ]
    
    print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(test_topics)} —Ç–µ–º...")
    print()
    
    turbo.start_time = time.time()
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤ –±–∞—Ç—á–∞—Ö
    for i in range(0, len(test_topics), 512):
        batch = test_topics[i:i+512]
        result = turbo.learn_batch(batch)
        
        print(f"–ë–∞—Ç—á {i//512 + 1}:")
        print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {result['processed']}")
        print(f"  –í—Ä–µ–º—è: {result['time']:.3f} —Å–µ–∫")
        print(f"  –°–∫–æ—Ä–æ—Å—Ç—å: {result['speed']:.1f} —Ç–µ–º/—Å–µ–∫")
        print(f"  GPU Memory: {result['gpu_memory_mb']:.0f} MB")
        print()
    
    stats = turbo.get_stats()
    print("="*80)
    print("–ò–¢–û–ì–ò:")
    print(f"  –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['total_processed']}")
    print(f"  –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {stats['speed']:.1f} —Ç–µ–º/—Å–µ–∫")
    print(f"  GPU: {stats['gpu_info']['gpu_name']}")
    print(f"  VRAM: {stats['gpu_info']['memory_allocated']:.0f} MB")
    print("="*80)
    print()
    print("‚ö†Ô∏è  –ü–†–û–í–ï–†–¨–¢–ï nvidia-smi - GPU –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ 90%+!")


if __name__ == "__main__":
    test_gpu_load()
