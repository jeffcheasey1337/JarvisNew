# -*- coding: utf-8 -*-
"""
üåç INFINITE AUTONOMOUS LEARNING SYSTEM
–ë–µ—Å–∫–æ–Ω–µ—á–Ω–∞—è –∞–≤—Ç–æ–Ω–æ–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- 300+ —è–∑—ã–∫–æ–≤ Wikipedia
- –í–µ—Å—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç (–Ω–æ–≤–æ—Å—Ç–∏, –±–ª–æ–≥–∏, —Ñ–æ—Ä—É–º—ã)
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç–µ–º
- –ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π
- –ë–µ—Å–∫–æ–Ω–µ—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
"""

import logging
import requests
from bs4 import BeautifulSoup
import time
import re
from pathlib import Path
import json
from collections import defaultdict, deque
from datetime import datetime
import hashlib

logger = logging.getLogger(__name__)


class MultilingualWikipediaCollector:
    """–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π —Å–±–æ—Ä—â–∏–∫ –∏–∑ Wikipedia —Å —É–º–Ω—ã–º –ø–æ–∏—Å–∫–æ–º"""
    
    # –¢–æ–ø-50 —è–∑—ã–∫–æ–≤ Wikipedia
    LANGUAGES = [
        'ru', 'en', 'de', 'fr', 'es', 'it', 'pl', 'ja', 'zh', 'pt',
        'nl', 'sv', 'ar', 'uk', 'fa', 'ca', 'sr', 'id', 'ko', 'no',
        'fi', 'hu', 'cs', 'tr', 'ro', 'vi', 'da', 'eo', 'sk', 'he',
        'bg', 'kk', 'eu', 'sl', 'hr', 'lt', 'et', 'az', 'gl', 'simple',
        'nn', 'la', 'el', 'th', 'sh', 'vo', 'hi', 'ta', 'ka', 'mk'
    ]
    
    # –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã –∏–º–µ–Ω/—Ç–µ—Ä–º–∏–Ω–æ–≤
    TRANSLATIONS = {
        '–ö–≤–µ–Ω—Ç–∏–Ω –¢–∞—Ä–∞–Ω—Ç–∏–Ω–æ': 'Quentin Tarantino',
        '–î–∂–æ—Ä–¥–∂ –ö–ª—É–Ω–∏': 'George Clooney',
        '–õ–µ–æ–Ω–∞—Ä–¥–æ –î–∏–ö–∞–ø—Ä–∏–æ': 'Leonardo DiCaprio',
        '–ú–∞—Ä—Ç–∏–Ω –°–∫–æ—Ä—Å–µ–∑–µ': 'Martin Scorsese',
        '–°—Ç–∏–≤–µ–Ω –°–ø–∏–ª–±–µ—Ä–≥': 'Steven Spielberg',
    }
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'JARVIS-Infinite-Learning/2.0 (Educational; Multilingual) Python/3.11'
        })
        
        self.stats = {
            'languages_used': set(),
            'articles_collected': 0,
            'fallback_searches': 0,
        }
    
    def search_all_languages(self, query, max_languages=10):
        """
        –£–º–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö —Å fallback
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            max_languages: –ú–∞–∫—Å–∏–º—É–º —è–∑—ã–∫–æ–≤
            
        Returns:
            List of results
        """
        results = []
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–ø—Ä–æ—Å–∞
        query_variants = self._generate_query_variants(query)
        
        logger.debug(f"–í–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–∏—Å–∫–∞: {query_variants[:3]}...")
        
        # –ü—Ä–æ–±—É–µ–º –∫–∞–∂–¥—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
        for variant in query_variants:
            # –ï—Å–ª–∏ —É–∂–µ –Ω–∞—à–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ - —Å—Ç–æ–ø
            if len(results) >= 3:
                break
            
            # –ü–æ–∏—Å–∫ –ø–æ —è–∑—ã–∫–∞–º
            for lang in self.LANGUAGES[:max_languages]:
                try:
                    result = self._search_wikipedia(variant, lang)
                    
                    if result:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ç–∞–∫–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –µ—â–µ –Ω–µ—Ç
                        if not any(r['url'] == result['url'] for r in results):
                            results.append(result)
                            self.stats['languages_used'].add(lang)
                            logger.debug(f"‚úì {lang.upper()}: {len(result['content'])} —Å–∏–º–≤–æ–ª–æ–≤")
                    
                    time.sleep(0.3)
                    
                    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –Ω–∞ —ç—Ç–æ–º —è–∑—ã–∫–µ - —Å–ª–µ–¥—É—é—â–∏–π —è–∑—ã–∫
                    if result:
                        break
                    
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ {lang}: {e}")
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏
            if len(results) == 0:
                time.sleep(0.5)
        
        self.stats['articles_collected'] += len(results)
        
        if len(results) == 0:
            logger.warning(f"–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è '{query}' –∏ –µ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤")
        
        return results
    
    def _generate_query_variants(self, query):
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞
        
        –ü—Ä–∏–º–µ—Ä:
        "–ö–≤–µ–Ω—Ç–∏–Ω –¢–∞—Ä–∞–Ω—Ç–∏–Ω–æ" ‚Üí
          - "–ö–≤–µ–Ω—Ç–∏–Ω –¢–∞—Ä–∞–Ω—Ç–∏–Ω–æ"
          - "Quentin Tarantino" (–ø–µ—Ä–µ–≤–æ–¥)
          - "–¢–∞—Ä–∞–Ω—Ç–∏–Ω–æ" (—Ñ–∞–º–∏–ª–∏—è)
          - "Tarantino"
        """
        variants = [query]
        
        # 1. –ü—Ä—è–º–æ–π –ø–µ—Ä–µ–≤–æ–¥ –µ—Å–ª–∏ –µ—Å—Ç—å
        if query in self.TRANSLATIONS:
            variants.append(self.TRANSLATIONS[query])
        
        # 2. –ï—Å–ª–∏ —ç—Ç–æ –∏–º—è (2+ —Å–ª–æ–≤–∞ —Å –∑–∞–≥–ª–∞–≤–Ω—ã—Ö)
        words = query.split()
        if len(words) >= 2 and all(w and w[0].isupper() for w in words):
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∞–º–∏–ª–∏—é
            variants.append(words[-1])
            
            # –ü—Ä–æ—Å—Ç–æ–π —Ç—Ä–∞–Ω—Å–ª–∏—Ç —Ä—É—Å—Å–∫–∏—Ö –∏–º–µ–Ω
            if self._is_cyrillic(query):
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
                translit = self._simple_translit(query)
                variants.append(translit)
                # –ò —Ç–æ–ª—å–∫–æ —Ñ–∞–º–∏–ª–∏—é
                variants.append(self._simple_translit(words[-1]))
        
        # 3. –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        seen = set()
        unique_variants = []
        for v in variants:
            if v not in seen:
                seen.add(v)
                unique_variants.append(v)
        
        return unique_variants
    
    def _is_cyrillic(self, text):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü–µ"""
        return bool(re.search('[–∞-—è–ê-–Ø]', text))
    
    def _simple_translit(self, text):
        """–ü—Ä–æ—Å—Ç–∞—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è —Ä—É—Å—Å–∫–æ–≥–æ"""
        translit_dict = {
            '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'yo',
            '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
            '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
            '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
            '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya',
            '–ê': 'A', '–ë': 'B', '–í': 'V', '–ì': 'G', '–î': 'D', '–ï': 'E', '–Å': 'Yo',
            '–ñ': 'Zh', '–ó': 'Z', '–ò': 'I', '–ô': 'Y', '–ö': 'K', '–õ': 'L', '–ú': 'M',
            '–ù': 'N', '–û': 'O', '–ü': 'P', '–†': 'R', '–°': 'S', '–¢': 'T', '–£': 'U',
            '–§': 'F', '–•': 'H', '–¶': 'Ts', '–ß': 'Ch', '–®': 'Sh', '–©': 'Sch',
            '–™': '', '–´': 'Y', '–¨': '', '–≠': 'E', '–Æ': 'Yu', '–Ø': 'Ya'
        }
        
        result = ''
        for char in text:
            result += translit_dict.get(char, char)
        
        return result
    
    def _search_wikipedia(self, query, lang):
        """–ü–æ–∏—Å–∫ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º —è–∑—ã–∫–µ"""
        try:
            api_url = f"https://{lang}.wikipedia.org/w/api.php"
            
            # –ü–æ–∏—Å–∫
            search_params = {
                'action': 'opensearch',
                'search': query,
                'limit': 1,
                'format': 'json'
            }
            
            response = self.session.get(api_url, params=search_params, timeout=10)
            response.raise_for_status()
            
            results = response.json()
            
            if len(results) < 2 or not results[1]:
                return None
            
            title = results[1][0]
            
            # –ö–æ–Ω—Ç–µ–Ω—Ç
            content_params = {
                'action': 'query',
                'prop': 'extracts',
                'exintro': True,
                'explaintext': True,
                'titles': title,
                'format': 'json'
            }
            
            response = self.session.get(api_url, params=content_params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            pages = data.get('query', {}).get('pages', {})
            
            for page_data in pages.values():
                extract = page_data.get('extract', '')
                
                if extract and len(extract) > 100:
                    return {
                        'source': f'Wikipedia ({lang})',
                        'title': title,
                        'url': f"https://{lang}.wikipedia.org/wiki/{title.replace(' ', '_')}",
                        'content': extract,
                        'lang': lang
                    }
            
            return None
        
        except Exception as e:
            return None


class WebCrawler:
    """–ö—Ä–∞—É–ª–µ—Ä –¥–ª—è —Å–±–æ—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        self.visited_urls = set()
    
    def crawl_search_results(self, query, max_results=5):
        """
        –ö—Ä–∞—É–ª–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç DuckDuckGo HTML (–±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫)
        """
        results = []
        
        try:
            # DuckDuckGo HTML –ø–æ–∏—Å–∫
            search_url = f"https://html.duckduckgo.com/html/?q={query.replace(' ', '+')}"
            
            response = self.session.get(search_url, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏
                links = soup.find_all('a', class_='result__url', limit=max_results)
                
                for link in links:
                    url = link.get('href')
                    
                    if url and url.startswith('http') and url not in self.visited_urls:
                        # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                        content = self._scrape_page(url)
                        
                        if content:
                            results.append({
                                'source': 'Web',
                                'url': url,
                                'content': content
                            })
                            
                            self.visited_urls.add(url)
                        
                        time.sleep(1)
        
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∫—Ä–∞—É–ª–∏–Ω–≥–∞: {e}")
        
        return results
    
    def _scrape_page(self, url):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω–æ–µ
            for tag in soup(['script', 'style', 'nav', 'header', 'footer']):
                tag.decompose()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            text = soup.get_text(separator=' ', strip=True)
            
            # –û—á–∏—Å—Ç–∫–∞
            text = re.sub(r'\s+', ' ', text)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º
            if len(text) > 5000:
                text = text[:5000] + "..."
            
            return text if len(text) > 200 else None
        
        except:
            return None


class EntityExtractor:
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
    
    # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –º—É—Å–æ—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    STOP_WORDS = {
        'the', 'and', 'for', 'with', 'from', 'this', 'that', 'these', 'those',
        '–≤', '–∏', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∫–∞–∫', '—á—Ç–æ', '—ç—Ç–æ', '–µ–≥–æ', '–µ—ë'
    }
    
    BAD_PATTERNS = [
        r'\d+',  # –¢–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã
        r'^[A-Z]{1,2}$',  # –û–¥–Ω–∞-–¥–≤–µ –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã
        r"'s$",  # –û–∫–æ–Ω—á–∞–Ω–∏—è 's
        r'tery$',  # Interpretery
        r'tory$',  # Territory
        r'^The\s',  # –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å The
    ]
    
    @classmethod
    def is_valid_entity(cls, text):
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ —Ç–µ–º—ã
        
        –§–∏–ª—å—Ç—Ä—É–µ—Ç:
        - –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ/–¥–ª–∏–Ω–Ω—ã–µ
        - –°—Ç–æ–ø-—Å–ª–æ–≤–∞
        - –ü–ª–æ—Ö–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        - –ú—É—Å–æ—Ä
        """
        if not text or len(text) < 3 or len(text) > 50:
            return False
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞
        if text.lower() in cls.STOP_WORDS:
            return False
        
        # –ü–ª–æ—Ö–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        for pattern in cls.BAD_PATTERNS:
            if re.search(pattern, text):
                return False
        
        # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        punct_count = sum(1 for c in text if not c.isalnum() and c not in [' ', '-'])
        if punct_count > 3:
            return False
        
        return True
    
    @classmethod
    def normalize_entity(cls, text):
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—É—â–Ω–æ—Å—Ç–∏
        
        "Monty Python's Flying Circus" ‚Üí "Monty Python"
        """
        # –£–±–∏—Ä–∞–µ–º –∞–ø–æ—Å—Ç—Ä–æ—Ñ—ã
        text = re.sub(r"'s\b", "", text)
        text = re.sub(r"'", "", text)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–µ–µ
        text = re.sub(r'\s+', ' ', text).strip()
        
        # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ - –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2-3 —Å–ª–æ–≤–∞
        words = text.split()
        if len(words) > 3:
            text = ' '.join(words[:3])
        
        return text
    
    @classmethod
    def extract_entities(cls, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        entities = {
            'people': set(),
            'places': set(),
            'topics': set(),
        }
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        words = text.split()
        
        # 1. –ù–∞—Ö–æ–¥–∏–º —Ñ—Ä–∞–∑—ã –∏–∑ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö —Å–ª–æ–≤ (–∏–º–µ–Ω–∞, –Ω–∞–∑–≤–∞–Ω–∏—è)
        i = 0
        while i < len(words):
            word = words[i]
            
            # –î–æ–ª–∂–Ω–æ –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –∏ –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–º
            if word and len(word) > 2 and word[0].isupper():
                # –ü—Ä–æ–±—É–µ–º —Å–æ–±—Ä–∞—Ç—å —Ñ—Ä–∞–∑—É
                phrase_words = [word]
                j = i + 1
                
                # –°–æ–±–∏—Ä–∞–µ–º –¥–æ 3 —Å–ª–æ–≤ –ø–æ–¥—Ä—è–¥ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π
                while j < len(words) and len(phrase_words) < 3:
                    next_word = words[j]
                    if next_word and next_word[0].isupper():
                        phrase_words.append(next_word)
                        j += 1
                    else:
                        break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã (–æ—Ç –¥–ª–∏–Ω–Ω–æ–π —Ñ—Ä–∞–∑—ã –∫ –∫–æ—Ä–æ—Ç–∫–æ–π)
                for length in range(len(phrase_words), 0, -1):
                    phrase = ' '.join(phrase_words[:length])
                    
                    if cls.is_valid_entity(phrase):
                        normalized = cls.normalize_entity(phrase)
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                        if length >= 2:
                            entities['people'].add(normalized)
                        else:
                            entities['topics'].add(normalized)
                        
                        i += length
                        break
                else:
                    i += 1
            else:
                i += 1
        
        # 2. –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ - —á–∞—Å—Ç–æ —É–ø–æ–º–∏–Ω–∞–µ–º—ã–µ —Å–ª–æ–≤–∞
        word_freq = defaultdict(int)
        for word in words:
            if len(word) > 4 and word[0].isupper():
                word_freq[word] += 1
        
        # –ë–µ—Ä–µ–º —á–∞—Å—Ç–æ —É–ø–æ–º–∏–Ω–∞–µ–º—ã–µ (2+ —Ä–∞–∑–∞)
        for word, freq in word_freq.items():
            if freq >= 2 and cls.is_valid_entity(word):
                normalized = cls.normalize_entity(word)
                entities['topics'].add(normalized)
        
        # 3. –í –∫–∞–≤—ã—á–∫–∞—Ö
        quotes = re.findall(r'"([^"]+)"', text)
        for quote in quotes:
            if cls.is_valid_entity(quote):
                normalized = cls.normalize_entity(quote)
                entities['topics'].add(normalized)
        
        return entities


class KnowledgeGraph:
    """–ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π - —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏"""
    
    def __init__(self):
        self.graph = defaultdict(set)  # topic -> set of related topics
        self.topic_info = {}  # topic -> metadata
    
    def add_topic(self, topic, metadata=None):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–º—ã"""
        if topic not in self.topic_info:
            self.topic_info[topic] = metadata or {}
    
    def add_relation(self, topic1, topic2):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏"""
        self.graph[topic1].add(topic2)
        self.graph[topic2].add(topic1)
    
    def get_related(self, topic, max_depth=2):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ç–µ–º"""
        if topic not in self.graph:
            return set()
        
        related = set()
        visited = set()
        queue = deque([(topic, 0)])
        
        while queue:
            current, depth = queue.popleft()
            
            if current in visited or depth > max_depth:
                continue
            
            visited.add(current)
            
            if current != topic:
                related.add(current)
            
            if depth < max_depth:
                for neighbor in self.graph[current]:
                    queue.append((neighbor, depth + 1))
        
        return related
    
    def save(self, filepath):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞"""
        data = {
            'graph': {k: list(v) for k, v in self.graph.items()},
            'info': self.topic_info
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def load(self, filepath):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≥—Ä–∞—Ñ–∞"""
        if Path(filepath).exists():
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                self.graph = defaultdict(set)
                for k, v in data.get('graph', {}).items():
                    self.graph[k] = set(v)
                
                self.topic_info = data.get('info', {})


class InfiniteLearningSystem:
    """
    –ë–ï–°–ö–û–ù–ï–ß–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –û–ë–£–ß–ï–ù–ò–Ø
    
    –ü–æ—Å—Ç–æ—è–Ω–Ω–æ —É—á–∏—Ç—Å—è, —Ä–∞—Å—à–∏—Ä—è–µ—Ç –∑–Ω–∞–Ω–∏—è, –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è
    """
    
    def __init__(self, turbo_system=None, initial_topics=None):
        self.turbo_system = turbo_system
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.wiki_collector = MultilingualWikipediaCollector()
        self.web_crawler = WebCrawler()
        self.entity_extractor = EntityExtractor()
        self.knowledge_graph = KnowledgeGraph()
        
        # –û—á–µ—Ä–µ–¥—å —Ç–µ–º –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è
        self.topic_queue = deque(initial_topics or [])
        self.studied_topics = set()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'start_time': datetime.now(),
            'topics_studied': 0,
            'sources_collected': 0,
            'entities_discovered': 0,
            'total_content': 0,
        }
        
        # –ü–∞–ø–∫–∏
        self.data_dir = Path('data/infinite_knowledge')
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≥—Ä–∞—Ñ –µ—Å–ª–∏ –µ—Å—Ç—å
        graph_file = self.data_dir / 'knowledge_graph.json'
        self.knowledge_graph.load(graph_file)
        
        logger.info("üåç Infinite Learning System –≥–æ—Ç–æ–≤–∞")
    
    def learn_topic(self, topic):
        """
        –ò–∑—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —Ç–µ–º—ã
        
        –ü—Ä–æ—Ü–µ—Å—Å:
        1. –£–º–Ω—ã–π –ø–æ–∏—Å–∫ –≤ Wikipedia (—Å –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏)
        2. –ö—Ä–∞—É–ª–∏–Ω–≥ –≤–µ–± (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        3. –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        4. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ–º
        5. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
        6. –°–æ–∑–¥–∞–Ω–∏–µ embeddings
        """
        if topic in self.studied_topics:
            logger.debug(f"–¢–µ–º–∞ '{topic}' —É–∂–µ –∏–∑—É—á–µ–Ω–∞")
            return False
        
        logger.info(f"üéì –ò–∑—É—á–µ–Ω–∏–µ: {topic}")
        
        all_content = []
        all_sources = []
        
        try:
            # 1. Wikipedia —Å —É–º–Ω—ã–º –ø–æ–∏—Å–∫–æ–º
            logger.info(f"üåç –£–º–Ω—ã–π –ø–æ–∏—Å–∫ –≤ Wikipedia...")
            wiki_results = self.wiki_collector.search_all_languages(topic, max_languages=10)
            
            for result in wiki_results:
                all_content.append(result['content'])
                all_sources.append(result)
            
            logger.info(f"‚úì Wikipedia: {len(wiki_results)} —è–∑—ã–∫–æ–≤, {sum(len(r['content']) for r in wiki_results)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # 2. –í–µ–±-–∫—Ä–∞—É–ª–∏–Ω–≥ (–µ—Å–ª–∏ Wikipedia –Ω–µ –Ω–∞—à–ª–∞ –º–Ω–æ–≥–æ)
            if len(all_content) < 2:
                logger.info(f"üåê –ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ...")
                web_results = self.web_crawler.crawl_search_results(topic, max_results=3)
                
                for result in web_results:
                    all_content.append(result['content'])
                    all_sources.append(result)
                
                if web_results:
                    logger.info(f"‚úì –í–µ–±: {len(web_results)} —Å—Ç—Ä–∞–Ω–∏—Ü")
            
            if not all_content:
                logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è '{topic}'")
                self.studied_topics.add(topic)  # –ü–æ–º–µ—á–∞–µ–º —á—Ç–æ–±—ã –Ω–µ –ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–Ω–æ–≤–∞
                return False
            
            # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç
            full_content = "\n\n".join(all_content)
            
            logger.info(f"üìä –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(full_content)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {len(all_sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
            
            # 4. –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
            logger.info(f"üß† –ê–Ω–∞–ª–∏–∑ —Å—É—â–Ω–æ—Å—Ç–µ–π...")
            entities = self.entity_extractor.extract_entities(full_content)
            
            new_topics = entities['people'] | entities['topics']
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Ç–µ–º—ã –≤ –æ—á–µ—Ä–µ–¥—å
            added_count = 0
            for new_topic in new_topics:
                if new_topic not in self.studied_topics and new_topic not in self.topic_queue:
                    self.topic_queue.append(new_topic)
                    added_count += 1
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑—å –≤ –≥—Ä–∞—Ñ
                    self.knowledge_graph.add_relation(topic, new_topic)
            
            if added_count > 0:
                logger.info(f"‚ú® –ù–∞–π–¥–µ–Ω–æ {added_count} –Ω–æ–≤—ã—Ö –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ–º")
                logger.debug(f"–ü—Ä–∏–º–µ—Ä—ã: {list(new_topics)[:5]}...")
            
            self.stats['entities_discovered'] += added_count
            
            # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            self._save_topic_data(topic, {
                'content': full_content,
                'sources': all_sources,
                'entities': {k: list(v) for k, v in entities.items()},
                'timestamp': datetime.now().isoformat()
            })
            
            # 6. –°–æ–∑–¥–∞–µ–º embeddings
            if self.turbo_system:
                try:
                    chunks = self._split_content(full_content)
                    chunks_ctx = [f"{topic}: {chunk}" for chunk in chunks]
                    
                    self.turbo_system.learn_batch(chunks_ctx, category="infinite")
                    
                    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} embeddings")
                
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ embeddings: {e}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.studied_topics.add(topic)
            self.stats['topics_studied'] += 1
            self.stats['sources_collected'] += len(all_sources)
            self.stats['total_content'] += len(full_content)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ
            self.knowledge_graph.save(self.data_dir / 'knowledge_graph.json')
            
            logger.info(f"‚úÖ –¢–µ–º–∞ '{topic}' –∏–∑—É—á–µ–Ω–∞!")
            
            return True
        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑—É—á–µ–Ω–∏—è '{topic}': {e}")
            return False
    
    def start_infinite_learning(self, max_topics=None):
        """
        –ó–∞–ø—É—Å–∫ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            max_topics: –ú–∞–∫—Å–∏–º—É–º —Ç–µ–º (None = –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ)
        """
        logger.info("="*80)
        logger.info("üåç –ó–ê–ü–£–°–ö –ë–ï–°–ö–û–ù–ï–ß–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø")
        logger.info("="*80)
        logger.info(f"–ù–∞—á–∞–ª—å–Ω—ã—Ö —Ç–µ–º –≤ –æ—á–µ—Ä–µ–¥–∏: {len(self.topic_queue)}")
        logger.info(f"–ò–∑—É—á–µ–Ω–æ —Ä–∞–Ω–µ–µ: {len(self.studied_topics)}")
        logger.info("="*80)
        
        topics_processed = 0
        
        try:
            while self.topic_queue:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
                if max_topics and topics_processed >= max_topics:
                    logger.info(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç: {max_topics} —Ç–µ–º")
                    break
                
                # –ë–µ—Ä–µ–º —Å–ª–µ–¥—É—é—â—É—é —Ç–µ–º—É
                topic = self.topic_queue.popleft()
                
                logger.info(f"\n[{topics_processed + 1}] –û—á–µ—Ä–µ–¥—å: {len(self.topic_queue)} | –ò–∑—É—á–µ–Ω–æ: {len(self.studied_topics)}")
                
                # –ò–∑—É—á–∞–µ–º
                success = self.learn_topic(topic)
                
                if success:
                    topics_processed += 1
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞–∂–¥—ã–µ 10 —Ç–µ–º
                    if topics_processed % 10 == 0:
                        self._print_stats()
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏
                time.sleep(3)
        
        except KeyboardInterrupt:
            logger.info("\n‚ö† –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        
        finally:
            self._print_final_stats()
    
    def _split_content(self, content, max_size=2000):
        """–†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        chunks = []
        paragraphs = content.split('\n\n')
        current = ""
        
        for para in paragraphs:
            if len(current) + len(para) < max_size:
                current += para + "\n\n"
            else:
                if current:
                    chunks.append(current.strip())
                current = para + "\n\n"
        
        if current:
            chunks.append(current.strip())
        
        return chunks
    
    def _save_topic_data(self, topic, data):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Ç–µ–º—ã"""
        try:
            filename = re.sub(r'[<>:"/\\|?*]', '_', topic)[:100] + '.json'
            filepath = self.data_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
    
    def _print_stats(self):
        """–ü–µ—á–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        elapsed = (datetime.now() - self.stats['start_time']).total_seconds()
        
        logger.info("="*80)
        logger.info("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        logger.info(f"–¢–µ–º –∏–∑—É—á–µ–Ω–æ: {self.stats['topics_studied']}")
        logger.info(f"–í –æ—á–µ—Ä–µ–¥–∏: {len(self.topic_queue)}")
        logger.info(f"–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {self.stats['sources_collected']}")
        logger.info(f"–ù–æ–≤—ã—Ö —Ç–µ–º –Ω–∞–π–¥–µ–Ω–æ: {self.stats['entities_discovered']}")
        logger.info(f"–ö–æ–Ω—Ç–µ–Ω—Ç–∞: {self.stats['total_content'] / 1024:.1f} KB")
        logger.info(f"–í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {elapsed / 60:.1f} –º–∏–Ω—É—Ç")
        logger.info(f"–°–∫–æ—Ä–æ—Å—Ç—å: {self.stats['topics_studied'] / (elapsed / 60):.1f} —Ç–µ–º/–º–∏–Ω")
        logger.info("="*80)
    
    def _print_final_stats(self):
        """–§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        logger.info("\n" + "="*80)
        logger.info("üèÅ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
        logger.info("="*80)
        
        self._print_stats()
        
        logger.info(f"\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —è–∑—ã–∫–∏: {', '.join(sorted(self.wiki_collector.stats['languages_used']))}")
        logger.info(f"–ò–∑—É—á–µ–Ω–æ —Ç–µ–º: {len(self.studied_topics)}")
        logger.info(f"–û—Å—Ç–∞–ª–æ—Å—å –≤ –æ—á–µ—Ä–µ–¥–∏: {len(self.topic_queue)}")
        logger.info("="*80)


# –¢–µ—Å—Ç
if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(levelname)s - %(message)s'
    )
    
    print("="*80)
    print("üåç INFINITE LEARNING SYSTEM - TEST")
    print("="*80)
    print()
    
    # –ù–∞—á–∞–ª—å–Ω—ã–µ —Ç–µ–º—ã
    initial_topics = [
        "Python",
        "–ö–≤–µ–Ω—Ç–∏–Ω –¢–∞—Ä–∞–Ω—Ç–∏–Ω–æ",
        "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
    ]
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏—Å—Ç–µ–º—É
    system = InfiniteLearningSystem(initial_topics=initial_topics)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 5 —Ç–µ–º–∞–º–∏ –¥–ª—è —Ç–µ—Å—Ç–∞)
    system.start_infinite_learning(max_topics=5)
    
    print("\n‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")
