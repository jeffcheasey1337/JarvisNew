# -*- coding: utf-8 -*-
"""
‚ö° HYBRID LEARNING SYSTEM - 10x-15x FASTER!
–ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å requests (–±–µ–∑ aiohttp)

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç requests (–Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç—Å—è Wikipedia)
‚úÖ ThreadPoolExecutor - 10-20 –ø–æ—Ç–æ–∫–æ–≤
‚úÖ –£–º–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
‚úÖ –í—Å–µ 4127 —Ç–µ–º —Å—Ä–∞–∑—É

–°–∫–æ—Ä–æ—Å—Ç—å: 50-100 —Ç–µ–º/–º–∏–Ω (10x-15x –±—ã—Å—Ç—Ä–µ–µ!)
4127 —Ç–µ–º: 40-80 –º–∏–Ω—É—Ç –≤–º–µ—Å—Ç–æ 10 —á–∞—Å–æ–≤!
"""

import logging
import requests
import time
import re
from pathlib import Path
import json
from collections import defaultdict, deque
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

logger = logging.getLogger(__name__)


class HybridWikipediaCollector:
    """–ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—ã–π —Å–±–æ—Ä—â–∏–∫ —Å requests"""
    
    LANGUAGES = ['ru', 'en']  # –ë–´–°–¢–†–û: —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π!
    
    TRANSLIT_MAP = {
        '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'yo',
        '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
        '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
        '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
        '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya',
    }
    
    def __init__(self):
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º pool size
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'JARVIS-Hybrid/1.0 (Educational; Multilingual) Python/3.11'
        })
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä connection pool –¥–æ 50
        adapter = HTTPAdapter(
            pool_connections=50,
            pool_maxsize=50,
            max_retries=Retry(total=3, backoff_factor=0.5)
        )
        
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
        
        self.stats = {
            'languages_used': set(),
            'articles_collected': 0,
        }
        
        self.lock = threading.Lock()
    
    @classmethod
    def transliterate(cls, text):
        """–ë—ã—Å—Ç—Ä–∞—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è"""
        return ''.join(cls.TRANSLIT_MAP.get(c.lower(), c) for c in text)
    
    @classmethod
    def generate_variants(cls, query):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        variants = [query]
        
        # –¢—Ä–∞–Ω—Å–ª–∏—Ç
        if re.search('[–∞-—è–ê-–Ø]', query):
            translit = cls.transliterate(query)
            if translit != query:
                variants.append(translit)
        
        # –ü–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ
        words = query.split()
        if len(words) > 1:
            variants.append(words[-1])
            if re.search('[–∞-—è–ê-–Ø]', words[-1]):
                variants.append(cls.transliterate(words[-1]))
        
        return list(dict.fromkeys(variants))
    
    def search_parallel(self, query, max_languages=5):
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–∞—Ö
        
        Returns:
            List of results
        """
        variants = self.generate_variants(query)
        
        results = []
        
        # –ü—Ä–æ–±—É–µ–º 2 –≤–∞—Ä–∏–∞–Ω—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞
        for variant in variants[:2]:
            # –ü–æ–∏—Å–∫ –Ω–∞ —è–∑—ã–∫–∞—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ (–Ω–æ –±—ã—Å—Ç—Ä–æ)
            for lang in self.LANGUAGES[:max_languages]:
                result = self._fetch_wikipedia(variant, lang)
                
                if result:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–µ –¥—É–±–ª–∏–∫–∞—Ç
                    if not any(r.get('url') == result.get('url') for r in results):
                        results.append(result)
                        
                        with self.lock:
                            self.stats['languages_used'].add(lang)
                        
                        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ 3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ - –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
                        if len(results) >= 3:
                            break
                
                # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏
                time.sleep(0.2)
            
            if len(results) >= 3:
                break
        
        with self.lock:
            self.stats['articles_collected'] += len(results)
        
        return results
    
    def _fetch_wikipedia(self, query, lang):
        """–ó–∞–ø—Ä–æ—Å –∫ Wikipedia API"""
        try:
            api_url = f"https://{lang}.wikipedia.org/w/api.php"
            
            # 1. –ü–æ–∏—Å–∫
            search_params = {
                'action': 'opensearch',
                'search': query,
                'limit': 1,
                'format': 'json'
            }
            
            response = self.session.get(api_url, params=search_params, timeout=15)
            
            if response.status_code != 200:
                return None
            
            data = response.json()
            
            if len(data) < 2 or not data[1]:
                return None
            
            title = data[1][0]
            
            # 2. –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_params = {
                'action': 'query',
                'prop': 'extracts',
                'exintro': True,
                'explaintext': True,
                'titles': title,
                'format': 'json'
            }
            
            response = self.session.get(api_url, params=content_params, timeout=15)
            
            if response.status_code != 200:
                return None
            
            data = response.json()
            pages = data.get('query', {}).get('pages', {})
            
            for page_data in pages.values():
                extract = page_data.get('extract', '')
                
                if extract and len(extract) > 100:
                    return {
                        'source': f'Wikipedia ({lang})',
                        'title': title,
                        'url': f"https://{lang}.wikipedia.org/wiki/{title.replace(' ', '_')}",
                        'content': extract,
                        'lang': lang
                    }
            
            return None
        
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ {lang}/{query}: {e}")
            return None


class FastEntityExtractor:
    """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    
    STOP_WORDS = {'the', 'and', 'for', 'with', '–≤', '–∏', '–Ω–∞', '—Å'}
    
    @classmethod
    def extract_fast(cls, text):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ"""
        entities = set()
        words = text.split()
        i = 0
        
        while i < len(words):
            word = words[i]
            
            if word and len(word) > 2 and word[0].isupper():
                phrase = [word]
                j = i + 1
                
                while j < len(words) and len(phrase) < 3:
                    next_word = words[j]
                    if next_word and next_word[0].isupper():
                        phrase.append(next_word)
                        j += 1
                    else:
                        break
                
                if len(phrase) >= 2:
                    entity = ' '.join(phrase)
                    entity = entity.replace("'s", "").replace("'", "").strip()
                    
                    if cls._is_valid(entity):
                        entities.add(entity)
                    
                    i = j
                else:
                    i += 1
            else:
                i += 1
        
        return entities
    
    @classmethod
    def _is_valid(cls, text):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è"""
        if not text or len(text) < 3 or len(text) > 50:
            return False
        if text.lower() in cls.STOP_WORDS:
            return False
        if text[0].isdigit():
            return False
        return True


class HybridLearningSystem:
    """
    –ì–ò–ë–†–ò–î–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –û–ë–£–ß–ï–ù–ò–Ø
    
    10x-15x –±—ã—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–π –≤–µ—Ä—Å–∏–∏!
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç ThreadPoolExecutor –≤–º–µ—Å—Ç–æ asyncio
    """
    
    def __init__(self, turbo_system=None, memory_system=None, topics_list=None, num_workers=15):
        self.turbo_system = turbo_system
        self.memory_system = memory_system  # –í–ê–ñ–ù–û: –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ø–∞–º—è—Ç—å
        self.num_workers = num_workers
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.wiki_collector = HybridWikipediaCollector()
        self.entity_extractor = FastEntityExtractor()
        
        # –û—á–µ—Ä–µ–¥–∏
        self.topic_queue = deque(topics_list or [])
        self.studied_topics = set()
        
        # –ì—Ä–∞—Ñ
        self.knowledge_graph = defaultdict(set)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'start_time': datetime.now(),
            'topics_studied': 0,
            'sources_collected': 0,
            'entities_discovered': 0,
            'total_content': 0,
        }
        
        # –ü–∞–ø–∫–∞
        self.data_dir = Path('data/hybrid_knowledge')
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # Batch –¥–ª—è embeddings
        self.embeddings_batch = []
        self.batch_size = 500
        self.lock = threading.Lock()
        
        logger.info(f"Hybrid Learning –≥–æ—Ç–æ–≤–∞ ({num_workers} –ø–æ—Ç–æ–∫–æ–≤)")
    
    def learn_topic(self, topic):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–µ–º–µ (–∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ)"""
        if topic in self.studied_topics:
            return False
        
        try:
            # –ü–æ–∏—Å–∫ - –ë–´–°–¢–†–´–ô (—Ç–æ–ª—å–∫–æ 2 —è–∑—ã–∫–∞!)
            wiki_results = self.wiki_collector.search_parallel(topic, max_languages=2)
            
            if not wiki_results:
                with self.lock:
                    self.studied_topics.add(topic)
                return False
            
            # –ö–æ–Ω—Ç–µ–Ω—Ç
            all_content = []
            for result in wiki_results:
                all_content.append(result['content'])
            
            full_content = "\n\n".join(all_content)
            
            # –°—É—â–Ω–æ—Å—Ç–∏
            entities = self.entity_extractor.extract_fast(full_content)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å
            added = 0
            with self.lock:
                for entity in entities:
                    if entity not in self.studied_topics and entity not in self.topic_queue:
                        self.topic_queue.append(entity)
                        self.knowledge_graph[topic].add(entity)
                        added += 1
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            self._save_fast(topic, {
                'content': full_content[:2000],
                'sources_count': len(wiki_results),
                'entities': list(entities)[:20],
            })
            
            # –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ memory_system!
            if self.memory_system:
                try:
                    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è –ø–∞–º—è—Ç–∏
                    chunks = self._split_fast(full_content, max_size=500)
                    
                    for chunk in chunks[:5]:  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 —á–∞–Ω–∫–æ–≤
                        self.memory_system.add_memory(
                            content=f"{topic}: {chunk}",
                            memory_type="knowledge",
                            metadata={
                                'topic': topic,
                                'source': 'wikipedia',
                                'auto_learned': True
                            }
                        )
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ø–∞–º—è—Ç—å: {e}")
            
            # Embeddings –≤ batch
            chunks = self._split_fast(full_content)
            with self.lock:
                for chunk in chunks:
                    self.embeddings_batch.append(f"{topic}: {chunk}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            with self.lock:
                self.studied_topics.add(topic)
                self.stats['topics_studied'] += 1
                self.stats['sources_collected'] += len(wiki_results)
                self.stats['total_content'] += len(full_content)
                self.stats['entities_discovered'] += added
            
            return True
        
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ {topic}: {e}")
            return False
    
    def process_embeddings_batch(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö embeddings"""
        with self.lock:
            if not self.embeddings_batch or not self.turbo_system:
                return
            
            batch = self.embeddings_batch.copy()
            self.embeddings_batch = []
        
        try:
            self.turbo_system.learn_batch(batch, category="hybrid")
            logger.info(f"‚úÖ GPU: {len(batch)} embeddings")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ GPU: {e}")
    
    def start_hybrid_learning(self):
        """–ó–∞–ø—É—Å–∫ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        logger.info("="*80)
        logger.info(f"‚ö° HYBRID LEARNING - {self.num_workers} –ü–û–¢–û–ö–û–í")
        logger.info("="*80)
        logger.info(f"–í—Å–µ–≥–æ —Ç–µ–º: {len(self.topic_queue)}")
        logger.info("="*80)
        
        total_topics = len(self.topic_queue)
        processed = 0
        
        try:
            with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
                while self.topic_queue:
                    # –ë–µ—Ä–µ–º batch
                    batch = []
                    for _ in range(min(self.num_workers * 2, len(self.topic_queue))):
                        if self.topic_queue:
                            batch.append(self.topic_queue.popleft())
                    
                    if not batch:
                        break
                    
                    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
                    futures = {executor.submit(self.learn_topic, topic): topic for topic in batch}
                    
                    # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                    for future in as_completed(futures):
                        processed += 1
                        
                        # –ö–∞–∂–¥—ã–µ 100 —Ç–µ–º - embeddings
                        if len(self.embeddings_batch) >= self.batch_size:
                            self.process_embeddings_batch()
                        
                        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞–∂–¥—ã–µ 100 —Ç–µ–º
                        if processed % 100 == 0:
                            self._print_stats(processed, total_topics)
        
        except KeyboardInterrupt:
            logger.info("\n‚ö† –û—Å—Ç–∞–Ω–æ–≤–∫–∞")
        
        finally:
            # –û—Å—Ç–∞–≤—à–∏–µ—Å—è embeddings
            if self.embeddings_batch:
                self.process_embeddings_batch()
            
            self._print_final_stats(total_topics)
    
    def _split_fast(self, content, max_size=1500):
        """–ë—ã—Å—Ç—Ä–∞—è —Ä–∞–∑–±–∏–≤–∫–∞"""
        chunks = []
        for i in range(0, len(content), max_size):
            chunks.append(content[i:i+max_size])
        return chunks
    
    def _save_fast(self, topic, data):
        """–ë—ã—Å—Ç—Ä–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ"""
        try:
            filename = re.sub(r'[<>:"/\\|?*]', '_', topic)[:100] + '.json'
            filepath = self.data_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False)
        except:
            pass
    
    def _print_stats(self, processed, total):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        elapsed = (datetime.now() - self.stats['start_time']).total_seconds()
        speed = self.stats['topics_studied'] / elapsed if elapsed > 0 else 0
        remaining = total - processed
        eta_seconds = remaining / speed if speed > 0 else 0
        
        logger.info("="*80)
        logger.info(f"‚ö° [{processed}/{total}] {(processed/total*100):.1f}%")
        logger.info(f"–ò–∑—É—á–µ–Ω–æ: {self.stats['topics_studied']}")
        logger.info(f"–í –æ—á–µ—Ä–µ–¥–∏: {len(self.topic_queue)}")
        logger.info(f"–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {self.stats['sources_collected']}")
        logger.info(f"–ù–æ–≤—ã—Ö —Ç–µ–º: {self.stats['entities_discovered']}")
        logger.info(f"–°–∫–æ—Ä–æ—Å—Ç—å: {speed*60:.0f} —Ç–µ–º/–º–∏–Ω")
        logger.info(f"ETA: {eta_seconds/60:.1f} –º–∏–Ω—É—Ç")
        logger.info("="*80)
    
    def _print_final_stats(self, total):
        """–§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        elapsed = (datetime.now() - self.stats['start_time']).total_seconds()
        
        logger.info("\n" + "="*80)
        logger.info("üèÅ HYBRID LEARNING –ó–ê–í–ï–†–®–ï–ù–û")
        logger.info("="*80)
        logger.info(f"–í—Å–µ–≥–æ —Ç–µ–º: {total}")
        logger.info(f"–ò–∑—É—á–µ–Ω–æ: {self.stats['topics_studied']}")
        logger.info(f"–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {self.stats['sources_collected']}")
        logger.info(f"–ù–æ–≤—ã—Ö —Ç–µ–º: {self.stats['entities_discovered']}")
        logger.info(f"–ö–æ–Ω—Ç–µ–Ω—Ç–∞: {self.stats['total_content']/1024/1024:.1f} MB")
        logger.info(f"–í—Ä–µ–º—è: {elapsed/60:.1f} –º–∏–Ω—É—Ç")
        logger.info(f"–°–∫–æ—Ä–æ—Å—Ç—å: {self.stats['topics_studied']/(elapsed/60):.0f} —Ç–µ–º/–º–∏–Ω")
        logger.info(f"–Ø–∑—ã–∫–æ–≤: {', '.join(sorted(self.wiki_collector.stats['languages_used']))}")
        logger.info("="*80)


# –¢–µ—Å—Ç
if __name__ == "__main__":
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(levelname)s - %(message)s'
    )
    
    print("="*80)
    print("‚ö° HYBRID LEARNING SYSTEM - TEST")
    print("="*80)
    print()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–º—ã
    try:
        import sys
        sys.path.insert(0, '.')
        from jarvis.core.learning.topics_database import get_all_topics_flat
        
        all_topics = get_all_topics_flat()
        print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_topics)} —Ç–µ–º –∏–∑ –±–∞–∑—ã")
    except:
        all_topics = ["Python", "Machine Learning", "AI"] * 10
        print(f"–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–º—ã: {len(all_topics)}")
    
    print()
    print(f"–ü–æ—Ç–æ–∫–æ–≤: 15")
    print(f"–û–∂–∏–¥–∞–µ–º–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å: 50-100 —Ç–µ–º/–º–∏–Ω")
    print(f"–í—Ä–µ–º—è –Ω–∞ {len(all_topics)} —Ç–µ–º: ~{len(all_topics)/75:.0f} –º–∏–Ω—É—Ç")
    print()
    
    input("Enter –¥–ª—è –∑–∞–ø—É—Å–∫–∞...")
    
    system = HybridLearningSystem(topics_list=all_topics[:100], num_workers=15)
    system.start_hybrid_learning()
    
    print("\n‚úÖ –ì–æ—Ç–æ–≤–æ!")
